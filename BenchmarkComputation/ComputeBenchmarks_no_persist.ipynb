{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b7eed2-1d38-40a6-825a-d9fc201f29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from pyspark.sql import SparkSession\n",
    "from time import time, sleep\n",
    "import subprocess\n",
    "import pprint\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "from internalLibrary.helperFunctions import *\n",
    "from internalLibrary.local import *\n",
    "from internalLibrary.parallel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2500614-e910-4c13-b4bd-033ecca6017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the spark warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "logging.getLogger('py4j').setLevel(logging.ERROR) \n",
    "logging.getLogger('pyspark').setLevel(logging.ERROR) \n",
    "log4j_conf_path = \"../Settings/log4j.properties\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fb5bf-cf09-44b6-81ec-4cedb6529c70",
   "metadata": {},
   "source": [
    "Global hyperparameters and data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "826cb1c4-f323-43b0-a155-f6dec45e08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files for reading and writing of data dictionaries\n",
    "pickle_fileP = 'dataP/log1P_noPersist.pkl' # Parallel initialization data\n",
    "pickle_fileR = 'dataR/log1U_noPersist.pkl' # Random initialization data\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "np.random.seed(12345)\n",
    "spark_seed = 54321\n",
    "\n",
    "# Number of partitions \n",
    "nSlices = [8]\n",
    "\n",
    "# Size of considered subset\n",
    "subLen = 40_000\n",
    "\n",
    "# Maximum number of iterations in Lloyds algorithm\n",
    "lloydsMaxIterations=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741807da-d71c-4192-973b-3207e1d23b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/09 09:22:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 8 number of partition started at time 1720516953.3755696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 6.812000274658203 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 944.7888958454132 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 8 number of partition ended at time 1720517918.7144194 after 965.3388497829437 seconds\n",
      "CPU times: user 4.1 s, sys: 799 ms, total: 4.9 s\n",
      "Wall time: 16min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    ### NAIVE INIT ###\\n    \\n    # Load log if it exists\\n    sleep(1)\\n    if os.path.isfile(pickle_fileR):\\n        with open(pickle_fileR, \"rb\") as f:\\n            logNaive = pickle.load(f)\\n            totalLogNaiveInit, totalLogNaiveKmeans, tDurationsNaive, tPreOperationsNaive = logNaive.values()\\n    else:\\n        totalLogNaiveInit = {}\\n        totalLogNaiveKmeans = {}\\n        tDurationsNaive = {}\\n        tPreOperationsNaive = {}\\n    \\n    # Start algo\\n    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\\n    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\\n    \\n    # Parallelize over nSlice partitions\\n    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\\n\\n    # Cut the categorical attributes\\n    Rdd = Rdd.map(deleteBytes)#.persist()\\n\\n    # Setting the theoretical number of clusters\\n    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])               .distinct()               .count()\\n    \\n    # Rescale the RDD over the max\\n    maxS = Rdd.map(lambda datum: datum[1][\"x\"])           .reduce(lambda a, b: np.maximum(a, b))\\n    minS = Rdd.map(lambda datum: datum[1][\"x\"])           .reduce(lambda a, b: np.minimum(a, b))\\n\\n    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))#.persist()\\n    \\n    # Setting up the input and output information for the algorithm\\n    logNaiveInit = {}\\n    logNaiveKmeans = {}\\n\\n    # Setup k and l\\n    k=kTrue\\n    l=k*2 \\n    \\n    tInitI = time()\\n\\n    tPreOperation = tInitI - tInit\\n    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\\n          \\n    # initialization kMeans//\\n    C_init = naiveInitFromSet(Rdd, k, logNaiveInit)\\n    \\n    tInitialization = time() - tInitI\\n    print(f\"Finished the initialization after {tInitialization} seconds\")\\n    \\n    # Run the k-means algorithm\\n    C = kMeans(Rdd, C_init, lloydsMaxIterations, logNaiveKmeans)\\n    \\n    # Time information\\n    tEnd = time() # compute the time of the end of the iteration over the number of partitions\\n    tDuration = tEnd - tInit\\n    \\n    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\\n\\n    # Output in the correct memory adresses\\n    totalLogNaiveInit[f\"Number of partition\" + str(nSlice)] = logNaiveInit\\n    totalLogNaiveKmeans[f\"Number of partition\" + str(nSlice)] = logNaiveKmeans\\n    tDurationsNaive[f\"Number of partition\" + str(nSlice)] = tDuration\\n    tPreOperationsNaive[f\"Number of partition\" + str(nSlice)] = tPreOperation\\n\\n    #Rdd.unpersist()\\n\\n    spark.catalog.clearCache() \\n    # for (id, rdd) in sc._jsc.getPersistentRDDs().items():\\n    #     rdd.unpersist()\\n    # print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\\n\\n    # Compute the total log\\n    logNaive = {\"totalLogNaiveInit\": totalLogNaiveInit, \"totalLogNaiveKmeans\": totalLogNaiveKmeans, \"tDurationsNaive\": tDurationsNaive, \"tPreOperationsNaive\": tPreOperationsNaive}\\n    \\n    # Save the log file\\n    if not os.path.exists(\\'dataR\\'): # create a directory if it doesnt exist\\n        os.makedirs(\\'dataR\\')\\n    \\n    with open(pickle_fileR, \"wb\") as filer:\\n        pickle.dump(logNaive, filer)\\n\\n    # Clear the space\\n    subprocess.run(\"ssh slave2 \\'cd /usr/local/spark/work/ && [ \"$(ls -A .)\" ] && rm -r ./*\\'\", shell=True)\\n    subprocess.run(\"ssh slave3 \\'cd /usr/local/spark/work/ && [ \"$(ls -A .)\" ] && rm -r ./*\\'\", shell=True)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### SPARK SETUP ###\n",
    "\n",
    "# Build a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"Clustering\")\\\n",
    "    .config(\"spark.executor.memory\", \"7g\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Dlog4j.configuration=file:{log4j_conf_path}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Eventually clear old data (if re-running)\n",
    "spark.catalog.clearCache() \n",
    "# for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "#     rdd.unpersist()\n",
    "\n",
    "#### IMPORT THE DATA SET ####\n",
    "data = fetch_kddcup99(return_X_y = True, percent10 = True) # default percent10=True\n",
    "\n",
    "# collect samples and features (target)\n",
    "x = data[0]\n",
    "y = data[1] \n",
    "\n",
    "# Shuffle\n",
    "shuffled_indices = np.random.permutation(len(x))\n",
    "x=x[shuffled_indices]\n",
    "y=y[shuffled_indices]\n",
    "\n",
    "# cut the data fro memory reasons\n",
    "x = x[:subLen,]\n",
    "y = y[:subLen]\n",
    "\n",
    "for nSlice in nSlices:\n",
    "    ### PARALLEL ###\n",
    "\n",
    "    # Open file if exists\n",
    "    sleep(1)\n",
    "    if os.path.isfile(pickle_fileP):\n",
    "        with open(pickle_fileP, \"rb\") as f:\n",
    "            logParallel = pickle.load(f)\n",
    "            totalLogParallelInit, totalLogParallelKmeans, tDurationsParallel, tPreOperationsParallel = logParallel.values()\n",
    "    else:\n",
    "        totalLogParallelInit = {}\n",
    "        totalLogParallelKmeans = {}\n",
    "        tDurationsParallel = {}\n",
    "        tPreOperationsParallel = {}\n",
    "\n",
    "    # Start the algorithm\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # Parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # Cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)#.persist()\n",
    "\n",
    "    # Setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # Rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))#.persist()\n",
    "    \n",
    "    # Setting up the input and output information for the algorithm\n",
    "    logParallelInit = {}\n",
    "    logParallelKmeans = {}\n",
    "\n",
    "    # Setup k and l\n",
    "    k=kTrue\n",
    "    l=k*2 \n",
    "    \n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # Initialization kMeans //\n",
    "    C_init = parallelInit(Rdd, k, l, logParallelInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # Run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, lloydsMaxIterations, logParallelKmeans)\n",
    "    \n",
    "    # Time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # Output in the correct memory adresses\n",
    "    totalLogParallelInit[f\"Number of partition\" + str(nSlice)] = logParallelInit\n",
    "    totalLogParallelKmeans[f\"Number of partition\" + str(nSlice)] = logParallelKmeans\n",
    "    tDurationsParallel[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsParallel[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    #Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    # for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    #     rdd.unpersist()\n",
    "    # print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "\n",
    "    # Compute the total log\n",
    "    logParallel = {\"totalLogParallelInit\": totalLogParallelInit, \"totalLogParallelKmeans\": totalLogParallelKmeans, \"tDurationsParallel\": tDurationsParallel, \"tPreOperationsParallel\": tPreOperationsParallel}\n",
    "    \n",
    "    # Save the log file\n",
    "    if not os.path.exists('dataP'): # create a directory if it doesnt exist\n",
    "        os.makedirs('dataP')\n",
    "    \n",
    "    with open(pickle_fileP, \"wb\") as file:\n",
    "        pickle.dump(logParallel, file)\n",
    "\n",
    "    # Clear the space\n",
    "    subprocess.run(\"ssh slave2 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "    subprocess.run(\"ssh slave3 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "\n",
    "\n",
    "    ### NAIVE INIT ###\n",
    "    \n",
    "    # Load log if it exists\n",
    "    sleep(1)\n",
    "    if os.path.isfile(pickle_fileR):\n",
    "        with open(pickle_fileR, \"rb\") as f:\n",
    "            logNaive = pickle.load(f)\n",
    "            totalLogNaiveInit, totalLogNaiveKmeans, tDurationsNaive, tPreOperationsNaive = logNaive.values()\n",
    "    else:\n",
    "        totalLogNaiveInit = {}\n",
    "        totalLogNaiveKmeans = {}\n",
    "        tDurationsNaive = {}\n",
    "        tPreOperationsNaive = {}\n",
    "    \n",
    "    # Start algo\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # Parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # Cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)#.persist()\n",
    "\n",
    "    # Setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # Rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))#.persist()\n",
    "    \n",
    "    # Setting up the input and output information for the algorithm\n",
    "    logNaiveInit = {}\n",
    "    logNaiveKmeans = {}\n",
    "\n",
    "    # Setup k and l\n",
    "    k=kTrue\n",
    "    l=k*2 \n",
    "    \n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans//\n",
    "    C_init = naiveInitFromSet(Rdd, k, logNaiveInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # Run the k-means algorithm\n",
    "    C = kMeans(Rdd, C_init, lloydsMaxIterations, logNaiveKmeans)\n",
    "    \n",
    "    # Time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # Output in the correct memory adresses\n",
    "    totalLogNaiveInit[f\"Number of partition\" + str(nSlice)] = logNaiveInit\n",
    "    totalLogNaiveKmeans[f\"Number of partition\" + str(nSlice)] = logNaiveKmeans\n",
    "    tDurationsNaive[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsNaive[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    #Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    # for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    #     rdd.unpersist()\n",
    "    # print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "    # Compute the total log\n",
    "    logNaive = {\"totalLogNaiveInit\": totalLogNaiveInit, \"totalLogNaiveKmeans\": totalLogNaiveKmeans, \"tDurationsNaive\": tDurationsNaive, \"tPreOperationsNaive\": tPreOperationsNaive}\n",
    "    \n",
    "    # Save the log file\n",
    "    if not os.path.exists('dataR'): # create a directory if it doesnt exist\n",
    "        os.makedirs('dataR')\n",
    "    \n",
    "    with open(pickle_fileR, \"wb\") as filer:\n",
    "        pickle.dump(logNaive, filer)\n",
    "\n",
    "    # Clear the space\n",
    "    subprocess.run(\"ssh slave2 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "    subprocess.run(\"ssh slave3 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59be8588-13da-4d15-b551-3b64462412b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill spark and the context\n",
    "sc.stop()\n",
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
