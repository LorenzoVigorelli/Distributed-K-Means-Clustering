{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b7eed2-1d38-40a6-825a-d9fc201f29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from pyspark.sql import SparkSession\n",
    "from time import time, sleep\n",
    "import subprocess\n",
    "import pprint\n",
    "import sys\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2500614-e910-4c13-b4bd-033ecca6017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the spark warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "logging.getLogger('py4j').setLevel(logging.ERROR) \n",
    "logging.getLogger('pyspark').setLevel(logging.ERROR) \n",
    "log4j_conf_path = \"file:///home/quivigorelli/Distributed-K-Means-Clustering/spark/DistributedKmeans/log4j.properties\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fb5bf-cf09-44b6-81ec-4cedb6529c70",
   "metadata": {},
   "source": [
    "Global hyperparameters and data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826cb1c4-f323-43b0-a155-f6dec45e08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files for reading and writing of data dictionaries\n",
    "pickle_fileP = 'dataP/log1P_Csafe.pkl' # Parallel initialization data\n",
    "pickle_fileR = 'dataR/log1U_Csafe.pkl' # Random initialization data\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "np.random.seed(12345)\n",
    "spark_seed = 54321\n",
    "\n",
    "# Number of partitions \n",
    "nSlices = [8]\n",
    "\n",
    "# Size of considered subset\n",
    "subLen = 40000\n",
    "\n",
    "# Maximum number of iterations in Lloyds algorithm\n",
    "lloydsMaxIterations=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1426166-735a-4146-9df4-3bf35d3e6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def labelToInt(label):\n",
    "    '''\n",
    "    Map from set of labels in original dataset (`strings`) into set of natural numbers (`int`) for easier manipulation of rdd\n",
    "    '''\n",
    "    uniqueLabels=list(np.unique(y))\n",
    "    return uniqueLabels.index(label)\n",
    "\n",
    "\n",
    "def deleteBytes(datum):\n",
    "    '''\n",
    "    Clean dataset from categorical attributes, leaving numerical ones\n",
    "    Arguments:\n",
    "    One datum of the rdd.\n",
    "    Return:\n",
    "    Updated datum.\n",
    "    '''\n",
    "    x = datum[1][\"x\"]\n",
    "    mask = [type(i) != bytes for i in x]\n",
    "    datum[1][\"x\"] = np.asarray(x[mask])\n",
    "    print(x)\n",
    "    print(mask)\n",
    "    return datum\n",
    "\n",
    "\n",
    "def localPlusPlusInit(points, k): \n",
    "    '''\n",
    "    KMeans++ initialization.\n",
    "    Arguments:\n",
    "    `points`: array (n, dim) of points to be clustered;\n",
    "    `k`: desired number of centroids. \n",
    "    Returns:\n",
    "    Initial array (k, dim) of centroids, k<=n.\n",
    "    '''\n",
    "    # Sample one point uniformly from points array\n",
    "    C=points[np.random.choice(points.shape[0])]\n",
    "    C=C[np.newaxis, :]\n",
    "    \n",
    "    for _ in range(k):\n",
    "        # Compute array (n,1) of probabilities associated to each point\n",
    "        probs=np.min(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1).flatten()\n",
    "        # Normalize probability distribution\n",
    "        probs=probs/np.sum(probs)\n",
    "        \n",
    "        # Draw one new centroid according to distrbution\n",
    "        nextCentroid=points[np.random.choice(points.shape[0], p=probs)][np.newaxis,:]\n",
    "        # Add centroid to array\n",
    "        C=np.vstack((C, nextCentroid))\n",
    "    return C\n",
    "\n",
    "\n",
    "def weightedAverage(group):\n",
    "    \"\"\"\n",
    "    Compute weighted average of a group from a pd.DataFrame with point coordinates, weights, clusterId.\n",
    "    Utilized in local (non-distributed) version of Lloyds algorithm, needed also in K-Means//\n",
    "    \"\"\"\n",
    "    weight_column='weights'\n",
    "    groupby_column='clusterId'\n",
    "    columns_to_average = group.columns.difference([weight_column, groupby_column])\n",
    "    weighted_averages = group[columns_to_average].multiply(group[weight_column], axis=0).sum() / group[weight_column].sum()\n",
    "    return weighted_averages\n",
    "\n",
    "\n",
    "def localLloyds(points, k, C_init=None, weights=None, n_iterations=100, logDict=None):\n",
    "    \"\"\"\n",
    "    Local (non-distributed) Lloyds algorithm\n",
    "    Arguments:\n",
    "    `points`: array (n, dim) of points to cluster;\n",
    "    `k`: number of desired clusters;\n",
    "    `C_init`: optional, array (k, dim) of initial centroids\n",
    "    `weights`: optional, weights for weighted average in centroid re-computing;\n",
    "    `n_iterations`: optional, number of iteration in lloyds algorithm;\n",
    "    `logDict`: optional, dictionary {'CostsKmeans', 'tIterations', 'tTotal'} to store cost and time info.\n",
    "    Return:\n",
    "    Array of expected centroids.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "\n",
    "    # Storing cost and time info\n",
    "    my_kMeansCosts = []\n",
    "    tIterations = []\n",
    "    \n",
    "    df=pd.DataFrame(points)\n",
    "\n",
    "    # If weights not given, assume uniform weights for points\n",
    "    if weights is None:\n",
    "        weights=np.ones(shape=len(points))\n",
    "    df['weights']=weights\n",
    "    df['clusterId']=np.zeros(shape=len(points))\n",
    "\n",
    "    # If no C_init, default to K-Means++ initialization\n",
    "    if C_init is None:\n",
    "        C=localPlusPlusInit(points, k)\n",
    "    else:\n",
    "        C=C_init\n",
    "   \n",
    "    clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "    for iteration in range(n_iterations):\n",
    "        t1=time()\n",
    "\n",
    "        # Compute centroid given cluster\n",
    "        df['clusterId']=clusterId\n",
    "        C_df=df.groupby('clusterId')\\\n",
    "            .apply(weightedAverage)\\\n",
    "            .reset_index()\n",
    "\n",
    "        # Compute cluster given centroid\n",
    "        C_array=C_df[C_df.columns.difference(['weights', 'clusterId'])].reset_index(drop=True).to_numpy()\n",
    "        squared_distances=np.sum((points[:,:,np.newaxis]-C_array.T[np.newaxis,:,:])**2, axis=1)\n",
    "        clusterId=np.argmin(squared_distances, axis=1)\n",
    "        my_cost=sum(squared_distances[np.arange(len(squared_distances)), clusterId])\n",
    "\n",
    "        my_kMeansCosts.append(my_cost)\n",
    "        t2 = time()\n",
    "        \n",
    "        tIteration = t2 - t1\n",
    "        tIterations.append(tIteration)\n",
    "\n",
    "    tEnd = time()\n",
    "    tTotal = tEnd - t0\n",
    "\n",
    "    # Store cost and time info\n",
    "    if logDict is not None:\n",
    "        logDict[\"CostsKmeans\"] = my_kMeansCosts\n",
    "        logDict[\"tIterations\"] = tIterations\n",
    "        logDict[\"tTotal\"] = tTotal\n",
    "    \n",
    "    return C_array \n",
    "\n",
    "\n",
    "def minmaxRescale(datum, minS, maxS):\n",
    "    \"\"\"\n",
    "    Rescale datum in [0,1] interval for better clusterization\n",
    "    Arguments:\n",
    "    `datum`: see rdd format;\n",
    "    `minS`: array of min coordinate value among points for each attribute;\n",
    "    `maxS`: as `minS` with max.\n",
    "    Return:\n",
    "    Updated datum.\n",
    "    \"\"\"\n",
    "    mask = np.array(minS < maxS).astype(bool)\n",
    "    feature = datum[1][\"x\"] \n",
    "    feature = (feature[mask] - minS[mask])/(maxS[mask] - minS[mask])\n",
    "    return (datum[0], {\"x\": feature, \"y\": datum[1][\"y\"], \"d2\":datum[1][\"d2\"]}) \n",
    "\n",
    "\n",
    "def selectCluster(datum, C, updateDistances=True):\n",
    "    \"\"\"\n",
    "    Associate datum to its centroid and optionally updates squared distance between them.\n",
    "    Arguments:\n",
    "    `datum`: see rdd format;\n",
    "    `C`: array (k, len(datum[1][\"x\"]));\n",
    "    `updateDistances`: if True, updates `datum[1][\"d2\"]` with squared distance between datum point and closest centroid in C.\n",
    "    Return:\n",
    "    Updated datum.\n",
    "    \"\"\"\n",
    "    distances = np.sum((datum[1][\"x\"] - C)**2, axis=1)\n",
    "    print('distances: ',distances)\n",
    "    clusterId = np.argmin(distances)\n",
    "    if updateDistances is True:\n",
    "        return (clusterId, {'x':datum[1]['x'], 'y':datum[1]['y'], 'd2':distances[clusterId]})\n",
    "    else:\n",
    "        return (clusterId, datum[1])\n",
    "\n",
    "\n",
    "def updateCentroids(Rdd):\n",
    "    \"\"\"\n",
    "    Update centroids as spatial average of cluster points\n",
    "    Argument:\n",
    "    `Rdd`: see rdd format;\n",
    "    Return:\n",
    "    Updated array of centroids.\n",
    "    \"\"\"\n",
    "    C=Rdd.mapValues(lambda xy: (xy['x'], 1))\\\n",
    "              .reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1]))\\\n",
    "              .mapValues(lambda a:a[0]/a[1])\\\n",
    "              .values()\\\n",
    "              .collect() \n",
    "    C=np.array(C) #check later more carefully if causes some overhead\n",
    "    return C\n",
    "\n",
    "\n",
    "def updateDistances(Rdd, C):\n",
    "    \"\"\"\n",
    "    Update Rdd with square distances from centroids, given Rdd with clusters already assigned to each point\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd format;\n",
    "    `C`: array of cluster centroids.\n",
    "    Return:\n",
    "    Updated rdd.\n",
    "    \"\"\"\n",
    "    def datumUpdate(datum, C):\n",
    "        '''\n",
    "        Update a datum of the rdd with distance from assigned centroid\n",
    "        '''\n",
    "        d2=np.sum((datum[1]['x']-C[datum[0]])**2)\n",
    "        #return datum\n",
    "        return (datum[0], {\"x\": datum[1][\"x\"], \"y\": datum[1][\"y\"], \"d2\":d2})\n",
    "    Rdd=Rdd.map(lambda datum:datumUpdate(datum, C))\n",
    "    return Rdd\n",
    "\n",
    "\n",
    "def cost(Rdd):\n",
    "    \"\"\"\n",
    "    Calculate global cost of clusterization, from an Rdd with distances from centroids already updated\n",
    "    \"\"\"\n",
    "    my_cost=Rdd.map(lambda datum : datum[1]['d2'])\\\n",
    "               .reduce(lambda a,b: a+b)\n",
    "    return my_cost \n",
    "\n",
    "\n",
    "def kMeans(Rdd, C_init, maxIterations, logParallelKmeans=None):\n",
    "    \"\"\"\n",
    "    Distributed (parallel) Lloyds algorithm\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd format;\n",
    "    `C_init`: array (k, dim) of initial centroids;\n",
    "    `maxIterations`: max number of iterations;\n",
    "    `logParallelKmeans`: optional, dictionary {'CostsKmeans', 'tIterations', 'tTotal'} to store cost and time info.\n",
    "    Return:\n",
    "    Array of expected centroids.\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time()\n",
    "\n",
    "    # Storing cost and time info\n",
    "    my_kMeansCosts = []\n",
    "    tIterations = []\n",
    "    C=C_init\n",
    "\n",
    "    for t in range(maxIterations):\n",
    "        t1 = time()\n",
    "        RddCached = Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "        \n",
    "        # Now we compute the new centroids by calculating the averages of points belonging to the same cluster.\n",
    "        C=updateCentroids(RddCached)\n",
    "        my_cost = cost(RddCached)\n",
    "        \n",
    "        my_kMeansCosts.append(my_cost)\n",
    "        t2 = time()\n",
    "        \n",
    "        tIteration = t2 - t1\n",
    "        tIterations.append(tIteration)\n",
    "        \n",
    "        RddCached.unpersist() \n",
    "\n",
    "        # Break loop if convergence of cost is reached\n",
    "        if (len(my_kMeansCosts) > 1) and (my_kMeansCosts[-1] > 0.999*my_kMeansCosts[-2]):\n",
    "            break\n",
    "\n",
    "    tEnd = time()\n",
    "    tTotal = tEnd - t0\n",
    "\n",
    "    # Store cost and time info in argument dictionary\n",
    "    if logParallelKmeans is not None:\n",
    "        logParallelKmeans[\"CostsKmeans\"] = my_kMeansCosts\n",
    "        logParallelKmeans[\"tIterations\"] = tIterations\n",
    "        logParallelKmeans[\"tTotal\"] = tTotal\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def naiveInitFromSet(Rdd, k, spark_seed=12345,logNaiveInit=None):\n",
    "    \"\"\"\n",
    "    Uniform sampling of k points from Rdd\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd structure;\n",
    "    `k`: desired number of clusters;\n",
    "    `spark_seed`: optional, seed for spark random sampling;\n",
    "    `logNaiveInit`: optional, dictionary {'tTotal'} to store time info.\n",
    "    Return:\n",
    "    Initial array (k, dim) of centroids.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    # Sampling. Replacement is set to False to avoid coinciding centroids BUT no guarantees that in the original dataset all points are distinct!!!\n",
    "    kSubset=Rdd.takeSample(False, k, seed=spark_seed)\n",
    "    C_init=np.array([datum[1]['x'] for datum in kSubset])\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logNaiveInit is not None:\n",
    "        logNaiveInit[\"tTotal\"] = tEnd - t0\n",
    "        \n",
    "    return C_init\n",
    "\n",
    "\n",
    "def naiveInitFromSpace(k, dim):\n",
    "    \"\"\"\n",
    "    Uniform drawing of k points from euclidean space assuming the Rdd has been mapped into a [0,1]^dim space\n",
    "    Arguments:\n",
    "    `k`: desired number of clusters;\n",
    "    `dim`: dimensionality of points space.\n",
    "    Return:\n",
    "    Initial array (k, dim) of centroids.\n",
    "    \"\"\"\n",
    "    C_init=np.random.uniform(size=(k,dim))\n",
    "    return C_init\n",
    "\n",
    "\n",
    "def parallelInit(Rdd, k, l, logParallelInit=None):\n",
    "    \"\"\"\n",
    "    Parallel initialization\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd structure;\n",
    "    `k`: desired number of clusters;\n",
    "    `l`: coefficient to adjust sampling probability in order to obtain at least k centroids;\n",
    "    `logParallelInit`: optional, dictionary {'CostsKmeans', 'tIterations', 'tTotal'} to store cost and time info.\n",
    "    Return:\n",
    "    Initial array (k, dim) of centroids.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    # initialize C as a point in the dataset\n",
    "    C=naiveInitFromSet(Rdd, 1) \n",
    "    \n",
    "    # associate each datum to the only centroid (computed before) and computed distances and cost\n",
    "    Rdd=Rdd.map(lambda datum : (0, datum[1]))\n",
    "    Rdd=updateDistances(Rdd, C).persist() ###\n",
    "    \n",
    "    my_cost=cost(Rdd)\n",
    "\n",
    "    # number of iterations (log(cost))\n",
    "    n_iterations=int(np.log(my_cost))\n",
    "    if(n_iterations<1): n_iterations=1\n",
    "    \n",
    "    tSamples = []\n",
    "    tCentroids = []\n",
    "    CostInits = [my_cost]\n",
    "    # iterative sampling of the centroids\n",
    "    for _ in range(n_iterations):\n",
    "\n",
    "        t1=time()\n",
    "        # sample C' according to the probability\n",
    "        C_prime=Rdd.filter(lambda datum : np.random.uniform()<l*datum[1]['d2']/my_cost)\\\n",
    "                   .map(lambda datum : datum[1]['x'])\\\n",
    "                   .collect()\n",
    "        C_prime=np.array(C_prime)\n",
    "        t2=time()\n",
    "\n",
    "        # stack C and C', update distances, centroids, and cost\n",
    "        if (C_prime.shape[0]>0):\n",
    "            C=np.vstack((C, C_prime))\n",
    "            \n",
    "            Rdd.unpersist() ###\n",
    "            Rdd=Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "            \n",
    "            my_cost=cost(Rdd)\n",
    "        t3=time()\n",
    "\n",
    "        tSample = t2 -t1\n",
    "        tCentroid = t3 - t2\n",
    "        tSamples.append(tSample)\n",
    "        tCentroids.append(tCentroid)\n",
    "        CostInits.append(my_cost)\n",
    "       \n",
    "    #erase centroids sampled more than once \n",
    "    C=C.astype(float)\n",
    "    C=np.unique(C, axis=0)\n",
    "    Rdd=Rdd.map(lambda datum: selectCluster(datum, C))\n",
    "    \n",
    "    #compute weights of centroids (sizes of each cluster) and put them in a list whose index is same centroid index as C\n",
    "    wx=Rdd.countByKey()\n",
    "    weights=np.zeros(len(C))\n",
    "    weights[[list(wx.keys())]]=[list(wx.values())]\n",
    "    \n",
    "    #subselection of k centroids from C, using local Lloyds algorithm with k-means++ initialization\n",
    "    if C.shape[0]<=k:\n",
    "        C_init=C\n",
    "    else:\n",
    "        C_init=localLloyds(C, k, weights=weights, n_iterations=100) #can be set to lloydsMaxIterations for consistency TODO\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logParallelInit is not None:\n",
    "        logParallelInit[\"tSamples\"] = tSamples\n",
    "        logParallelInit[\"tCentroids\"] = tCentroids\n",
    "        logParallelInit[\"CostInit\"] = CostInits\n",
    "        logParallelInit[\"tTotal\"] = tEnd - t0\n",
    "\n",
    "    Rdd.unpersist() ###\n",
    "    return C_init\n",
    "\n",
    "def predictedCentroidsLabeler(C_expected, C_predicted):\n",
    "    \"\"\"\n",
    "    Associate expected and predicted centroids based on distance.\n",
    "    Parameters:\n",
    "    `C_expected`: array (k, dim) of expected centroids;\n",
    "    `C_predicted`: array (k,dim) of predicted centroids;\n",
    "    Return:\n",
    "    List of labels, one for each expected centroid and pointing to its nearest predicted centroid;\n",
    "    List of corresponding distances.\n",
    "    \"\"\"\n",
    "    # Compute the distance matrix\n",
    "    distMatrix=np.sum((C_expected[:,:,np.newaxis]-C_predicted.T[np.newaxis, :,:])**2,axis=1)\n",
    "    # The labeler i-th entry j, tells that i-th centroid of C_expected is associated to j-th element of C_predicted\n",
    "    labeler=np.argmin(distMatrix,axis=1)\n",
    "    # Square distance of element of C_expected to nearest point in C_predicted\n",
    "    distances=np.sqrt(np.array(distMatrix[np.arange(len(distMatrix)),labeler]).astype(float))\n",
    "    return labeler, distances\n",
    "\n",
    "\n",
    "def nearestCentroidDistances(C):\n",
    "    \"\"\"\n",
    "    Associate each centroid to the distance of the nearest one\n",
    "    Parameters:\n",
    "    `C`:  array (k, dim) of centroids;\n",
    "    Return:\n",
    "    List of labels, one for each centroid and pointing to its nearest centroid;\n",
    "    List of corresponding distances.\n",
    "    \"\"\"\n",
    "    # Compute the distance matrix\n",
    "    distMatrix=np.sum((C[:,:,np.newaxis]-C.T[np.newaxis, :,:])**2,axis=1)\n",
    "    distMatrix+=np.diag(np.repeat(np.inf, distMatrix.shape[0]))\n",
    "    \n",
    "    # The labeler i-th entry j, tells that i-th centroid of C_expected is associated to j-th element of C_predicted\n",
    "    labeler=np.argmin(distMatrix,axis=1)\n",
    "    \n",
    "    # Square distance of element of C_expected to nearest point in C_predicted\n",
    "    distances=np.sqrt(np.array(distMatrix[np.arange(distMatrix.shape[0]),labeler]).astype(float))\n",
    "    return labeler, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741807da-d71c-4192-973b-3207e1d23b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 15:07:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 64 number of partition started at time 1720364857.4541245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 7.4231038093566895 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 101.80599570274353 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 64 number of partition ended at time 1720364980.0061474 after 122.55202293395996 seconds\n",
      "The iteration with 64 number of partition started at time 1720364981.840024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 4.6970415115356445 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 2.1025912761688232 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 64 number of partition ended at time 1720365016.2762065 after 34.436182498931885 seconds\n",
      "CPU times: user 4.04 s, sys: 685 ms, total: 4.73 s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### SPARK SETUP ###\n",
    "\n",
    "# Build a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"Clustering\")\\\n",
    "    .config(\"spark.executor.memory\", \"7g\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Dlog4j.configuration=file:{log4j_conf_path}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Eventually clear old data (if re-running)\n",
    "spark.catalog.clearCache() \n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    rdd.unpersist()\n",
    "\n",
    "#### IMPORT THE DATA SET ####\n",
    "data = fetch_kddcup99(return_X_y = True, percent10 = True) # default percent10=True\n",
    "\n",
    "# collect samples and features (target)\n",
    "x = data[0]\n",
    "y = data[1] \n",
    "\n",
    "# Shuffle\n",
    "shuffled_indices = np.random.permutation(len(x))\n",
    "x=x[shuffled_indices]\n",
    "y=y[shuffled_indices]\n",
    "\n",
    "# cut the data fro memory reasons\n",
    "x = x[:subLen,]\n",
    "y = y[:subLen]\n",
    "\n",
    "for nSlice in nSlices:\n",
    "    ### PARALLEL ###\n",
    "\n",
    "    # Open file if exists\n",
    "    sleep(1)\n",
    "    if os.path.isfile(pickle_fileP):\n",
    "        with open(pickle_fileP, \"rb\") as f:\n",
    "            logParallel = pickle.load(f)\n",
    "            totalLogParallelInit, totalLogParallelKmeans, tDurationsParallel, tPreOperationsParallel = logParallel.values()\n",
    "    else:\n",
    "        totalLogParallelInit = {}\n",
    "        totalLogParallelKmeans = {}\n",
    "        tDurationsParallel = {}\n",
    "        tPreOperationsParallel = {}\n",
    "\n",
    "    # Start the algorithm\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # Parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # Cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # Setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # Rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # Setting up the input and output information for the algorithm\n",
    "    logParallelInit = {}\n",
    "    logParallelKmeans = {}\n",
    "\n",
    "    # Setup k and l\n",
    "    k=kTrue\n",
    "    l=k*2 \n",
    "    \n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # Initialization kMeans //\n",
    "    C_init = parallelInit(Rdd, k, l, logParallelInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # Run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, lloydsMaxIterations, logParallelKmeans)\n",
    "    \n",
    "    # Time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # Output in the correct memory adresses\n",
    "    totalLogParallelInit[f\"Number of partition\" + str(nSlice)] = logParallelInit\n",
    "    totalLogParallelKmeans[f\"Number of partition\" + str(nSlice)] = logParallelKmeans\n",
    "    tDurationsParallel[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsParallel[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "    # print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "\n",
    "    # Compute the total log\n",
    "    logParallel = {\"totalLogParallelInit\": totalLogParallelInit, \"totalLogParallelKmeans\": totalLogParallelKmeans, \"tDurationsParallel\": tDurationsParallel, \"tPreOperationsParallel\": tPreOperationsParallel}\n",
    "    \n",
    "    # Save the log file\n",
    "    if not os.path.exists('dataP'): # create a directory if it doesnt exist\n",
    "        os.makedirs('dataP')\n",
    "    \n",
    "    with open(pickle_fileP, \"wb\") as file:\n",
    "        pickle.dump(logParallel, file)\n",
    "\n",
    "    # Clear the space\n",
    "    subprocess.run(\"ssh slave2 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "    subprocess.run(\"ssh slave3 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "\n",
    "\n",
    "    ### NAIVE INIT ###\n",
    "    \n",
    "    # Load log if it exists\n",
    "    sleep(1)\n",
    "    if os.path.isfile(pickle_fileR):\n",
    "        with open(pickle_fileR, \"rb\") as f:\n",
    "            logNaive = pickle.load(f)\n",
    "            totalLogNaiveInit, totalLogNaiveKmeans, tDurationsNaive, tPreOperationsNaive = logNaive.values()\n",
    "    else:\n",
    "        totalLogNaiveInit = {}\n",
    "        totalLogNaiveKmeans = {}\n",
    "        tDurationsNaive = {}\n",
    "        tPreOperationsNaive = {}\n",
    "    \n",
    "    # Start algo\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # Parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # Cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # Setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # Rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # Setting up the input and output information for the algorithm\n",
    "    logNaiveInit = {}\n",
    "    logNaiveKmeans = {}\n",
    "\n",
    "    # Setup k and l\n",
    "    k=kTrue\n",
    "    l=k*2 \n",
    "    \n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans//\n",
    "    C_init = naiveInitFromSet(Rdd, k, logNaiveInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # Run the k-means algorithm\n",
    "    C = kMeans(Rdd, C_init, lloydsMaxIterations, logNaiveKmeans)\n",
    "    \n",
    "    # Time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # Output in the correct memory adresses\n",
    "    totalLogNaiveInit[f\"Number of partition\" + str(nSlice)] = logNaiveInit\n",
    "    totalLogNaiveKmeans[f\"Number of partition\" + str(nSlice)] = logNaiveKmeans\n",
    "    tDurationsNaive[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsNaive[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "    # print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "    # Compute the total log\n",
    "    logNaive = {\"totalLogNaiveInit\": totalLogNaiveInit, \"totalLogNaiveKmeans\": totalLogNaiveKmeans, \"tDurationsNaive\": tDurationsNaive, \"tPreOperationsNaive\": tPreOperationsNaive}\n",
    "    \n",
    "    # Save the log file\n",
    "    if not os.path.exists('dataR'): # create a directory if it doesnt exist\n",
    "        os.makedirs('dataR')\n",
    "    \n",
    "    with open(pickle_fileR, \"wb\") as filer:\n",
    "        pickle.dump(logNaive, filer)\n",
    "\n",
    "    # Clear the space\n",
    "    subprocess.run(\"ssh slave2 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "    subprocess.run(\"ssh slave3 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59be8588-13da-4d15-b551-3b64462412b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill spark and the context\n",
    "sc.stop()\n",
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
