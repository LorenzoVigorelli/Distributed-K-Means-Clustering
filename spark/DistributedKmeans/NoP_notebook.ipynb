{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44221217-17fd-418a-9ce2-bead05f58c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e840b600-ca65-4950-952d-1fade8c9614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "356820d8-58d4-4c05-9eae-0f714778d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "logging.getLogger('pyspark').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2848fd43-970e-40d6-bdd8-864a29316676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95b7eed2-1d38-40a6-825a-d9fc201f29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORT LIBRARIES #### \n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from functions import *\n",
    "from time import time\n",
    "\n",
    "np.random.seed(12345)\n",
    "spark_seed = 54321\n",
    "\n",
    "\n",
    "timeString=str(time())\n",
    "#### SELECT THE PICKLE FILE ####\n",
    "pickle_fileP = 'dataP/log1P'+'_'+timeString+'.pkl'\n",
    "pickle_fileR = 'dataR/log1U'+'_'+timeString+'.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1426166-735a-4146-9df4-3bf35d3e6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FUNCTIONS #######\n",
    "def labelToInt(label):\n",
    "    \"\"\"\n",
    "    Let's define a map from Y (set of strings) into (0,size(Y)) for easier usage\n",
    "    \"\"\"\n",
    "    uniqueLabels=list(np.unique(y))\n",
    "    return uniqueLabels.index(label)\n",
    "\n",
    "\n",
    "def deleteBytes(datum):\n",
    "    x = datum[1][\"x\"]\n",
    "    mask = [type(i) != bytes for i in x]\n",
    "    datum[1][\"x\"] = np.asarray(x[mask])\n",
    "    print(x)\n",
    "    print(mask)\n",
    "    return datum\n",
    "    \n",
    "def localPlusPlusInit(points, k): \n",
    "    #print('pointsshape: ', points.shape)\n",
    "    '''\n",
    "    Initialization kmeans ++\n",
    "    points is a numpy array (n,dim)\n",
    "    '''\n",
    "    C=points[np.random.choice(points.shape[0])]#sample from array di punti ecc...\n",
    "    C=C[np.newaxis, :]\n",
    "    for _ in range(k):\n",
    "        #points is array (n, dim), C is array(g<=k, dim)\n",
    "        #probs is array (n,1)\n",
    "        probs=np.min(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1).flatten()\n",
    "        #probs=[min([sum((point-centroid)**2) for centroid in C]) for point in points] #numpyfy this, or numbafy if left base python\n",
    "        probs=probs/np.sum(probs)\n",
    "        nextCentroid=points[np.random.choice(points.shape[0], p=probs)][np.newaxis,:]\n",
    "        #print('LE FORME',C.shape, nextCentroid.shape)\n",
    "        C=np.vstack((C, nextCentroid))\n",
    "    return C\n",
    "\n",
    "\n",
    "def weightedAverage(group):\n",
    "    \"\"\"\n",
    "    Function to compute the weighted average\n",
    "    \"\"\"\n",
    "    weight_column='weights'\n",
    "    groupby_column='clusterId'\n",
    "    columns_to_average = group.columns.difference([weight_column, groupby_column])\n",
    "    weighted_averages = group[columns_to_average].multiply(group[weight_column], axis=0).sum() / group[weight_column].sum()\n",
    "    return weighted_averages\n",
    "\n",
    "\n",
    "def localLloyds(points, k, weights=None, n_iterations=100):\n",
    "    \"\"\"\n",
    "    function that does the Local Lloyds algorithm\n",
    "    \"\"\"\n",
    "    df=pd.DataFrame(points)\n",
    "    if weights is None:\n",
    "        weights=np.ones(shape=len(points))\n",
    "    #print('weights', weights)\n",
    "    df['weights']=weights\n",
    "    df['clusterId']=np.zeros(shape=len(points))\n",
    "    C=localPlusPlusInit(points, k)\n",
    "    #print('localPlusPluisInit: ', C)\n",
    "    clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "    for iteration in range(n_iterations):\n",
    "        df['clusterId']=clusterId\n",
    "        C_df=df.groupby('clusterId')\\\n",
    "            .apply(weightedAverage)\\\n",
    "            .reset_index()\n",
    "        C_array=C_df[C_df.columns.difference(['weights', 'clusterId'])].reset_index(drop=True).to_numpy()\n",
    "        clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C_array.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "        #print(clusterId)\n",
    "        \n",
    "    return C_array   \n",
    "\n",
    "\n",
    "def minmaxRescale(datum, minS, maxS):\n",
    "    \"\"\"\n",
    "    Rescale a datum in [0,1]\n",
    "    \"\"\"\n",
    "    mask = (minS < maxS).astype(bool)\n",
    "    feature = datum[1][\"x\"] \n",
    "    feature = (feature[mask] - minS[mask])/(maxS[mask] - minS[mask])\n",
    "    return (datum[0], {\"x\": feature, \"y\": datum[1][\"y\"], \"d2\":datum[1][\"d2\"]}) \n",
    "\n",
    "\n",
    "def selectCluster(datum, C, updateDistances=True):\n",
    "    \"\"\"\n",
    "    Associates a datum to its centroid and updates the distance if True\n",
    "    dimC(k, len(datum))\n",
    "    \"\"\"\n",
    "    distances = np.sum((datum[1][\"x\"] - C)**2, axis=1)\n",
    "    print('distances: ',distances)\n",
    "    clusterId = np.argmin(distances)\n",
    "    if updateDistances is True:\n",
    "        return (clusterId, {'x':datum[1]['x'], 'y':datum[1]['y'], 'd2':distances[clusterId]})\n",
    "    else:\n",
    "        return (clusterId, datum[1])\n",
    "\n",
    "\n",
    "def updateCentroids(Rdd):\n",
    "    \"\"\"\n",
    "    update centroids as 'centers of mass' of clusters\n",
    "    \"\"\"\n",
    "    C=Rdd.mapValues(lambda xy: (xy['x'], 1))\\\n",
    "              .reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1]))\\\n",
    "              .mapValues(lambda a:a[0]/a[1])\\\n",
    "              .values()\\\n",
    "              .collect() \n",
    "    C=np.array(C) #check later more carefully if causes some overhead\n",
    "    return C\n",
    "\n",
    "\n",
    "def updateDistances(Rdd, C):\n",
    "    \"\"\"\n",
    "    update the Rdd with square distances from centroids, given Rdd with centroids already updated\n",
    "    \"\"\"\n",
    "    def datumUpdate(datum, C):\n",
    "        d2=np.sum((datum[1]['x']-C[datum[0]])**2)\n",
    "        #return datum\n",
    "        return (datum[0], {\"x\": datum[1][\"x\"], \"y\": datum[1][\"y\"], \"d2\":d2})\n",
    "    Rdd=Rdd.map(lambda datum:datumUpdate(datum, C))\n",
    "    return Rdd\n",
    "\n",
    "\n",
    "def cost(Rdd):\n",
    "    \"\"\"\n",
    "    calculate global cost of X,C from an Rdd with distances from centroids already updated\n",
    "    \"\"\"\n",
    "    my_cost=Rdd.map(lambda datum : datum[1]['d2'])\\\n",
    "               .reduce(lambda a,b: a+b)\n",
    "    return my_cost \n",
    "\n",
    "\n",
    "def kMeans(Rdd, C_init, maxIterations, logParallelKmeans=None):\n",
    "    \"\"\"\n",
    "    kMeans in parallel (?)\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    my_kMeansCosts = []\n",
    "    tIterations = []\n",
    "    C=C_init\n",
    "\n",
    "    for t in range(maxIterations):\n",
    "        t1 = time()\n",
    "        RddCached = Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "        \n",
    "        # Now we compute the new centroids by calculating the averages of points belonging to the same cluster.\n",
    "        # Need to check that all centroids are assigned to at least one point, otherwise k changes!!! Solutions?!\n",
    "        C=updateCentroids(RddCached)\n",
    "        my_cost = cost(RddCached)\n",
    "        \n",
    "        my_kMeansCosts.append(my_cost)\n",
    "        t2 = time()\n",
    "        \n",
    "        tIteration = t2 - t1\n",
    "        tIterations.append(tIteration)\n",
    "        \n",
    "        RddCached.unpersist() ### bad for time efficiency, not necessary due to Python Garbage collector (?)\n",
    "        \n",
    "\n",
    "    tEnd = time()\n",
    "    tTotal = tEnd - t0\n",
    "    \n",
    "    if logParallelKmeans is not None:\n",
    "        logParallelKmeans[\"CostsKmeans\"] = my_kMeansCosts\n",
    "        logParallelKmeans[\"tIterations\"] = tIterations\n",
    "        logParallelKmeans[\"tTotal\"] = tTotal\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def naiveInitFromSet(Rdd, k, logNaiveInit=None):\n",
    "    \"\"\"\n",
    "    uniform sampling of k points from Rdd\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    kSubset=Rdd.takeSample(False, k, seed=spark_seed)\n",
    "    # Replacement is set to False to avoid coinciding centroids BUT no guarantees that in the original dataset all points are distinct!!! Check if causes problems in the algorithm (i.e. need to pre-filter) or it's ok\n",
    "    C_init=np.array([datum[1]['x'] for datum in kSubset])\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logNaiveInit is not None:\n",
    "        logNaiveInit[\"tTotal\"] = tEnd - t0\n",
    "        \n",
    "    return C_init\n",
    "\n",
    "\n",
    "def naiveInitFromSpace(k, dim):\n",
    "    \"\"\"\n",
    "    #uniform drawing of k points from euclidean space\n",
    "    #we assume the Rdd has been mapped into a [0,1]^dim space\n",
    "    \"\"\"\n",
    "    C_init=np.random.uniform(size=(k,dim))\n",
    "    return C_init\n",
    "\n",
    "\n",
    "def parallelInit(Rdd, k, l, logParallelInit=None):\n",
    "    \"\"\"\n",
    "    Parallel initialization\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    # initialize C as a point in the dataset\n",
    "    C=naiveInitFromSet(Rdd, 1) \n",
    "    \n",
    "    # associate each datum to the only centroid (computed before) and computed distances and cost\n",
    "    Rdd=Rdd.map(lambda datum : (0, datum[1]))\n",
    "    Rdd=updateDistances(Rdd, C).persist() ###\n",
    "    \n",
    "    my_cost=cost(Rdd)\n",
    "\n",
    "    # number of iterations (log(cost))\n",
    "    n_iterations=int(np.log(my_cost))\n",
    "    if(n_iterations<1): n_iterations=1\n",
    "\n",
    "    \n",
    "    tSamples = []\n",
    "    tCentroids = []\n",
    "    CostInits = [my_cost]\n",
    "    # iterative sampling of the centroids\n",
    "    for _ in range(n_iterations):\n",
    "\n",
    "        t1=time()\n",
    "        # sample C' according to the probability\n",
    "        C_prime=Rdd.filter(lambda datum : np.random.uniform()<l*datum[1]['d2']/my_cost)\\\n",
    "                   .map(lambda datum : datum[1]['x'])\\\n",
    "                   .collect()\n",
    "        C_prime=np.array(C_prime)\n",
    "        t2=time()\n",
    "\n",
    "        # stack C and C', update distances, centroids, and cost\n",
    "        if (C_prime.shape[0]>0):\n",
    "            C=np.vstack((C, C_prime))\n",
    "            \n",
    "            Rdd.unpersist() ###\n",
    "            Rdd=Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "            \n",
    "            my_cost=cost(Rdd)\n",
    "        t3=time()\n",
    "\n",
    "        tSample = t2 -t1\n",
    "        tCentroid = t3 - t2\n",
    "        tSamples.append(tSample)\n",
    "        tCentroids.append(tCentroid)\n",
    "        CostInits.append(my_cost)\n",
    "       \n",
    "    #erase centroids sampled more than once \n",
    "    C=C.astype(float)\n",
    "    C=np.unique(C, axis=0)\n",
    "    Rdd=Rdd.map(lambda datum: selectCluster(datum, C))\n",
    "    \n",
    "    #compute weights of centroids (sizes of each cluster) and put them in a list whose index is same centroid index as C\n",
    "    wx=Rdd.countByKey()\n",
    "    weights=np.zeros(len(C))\n",
    "    weights[[list(wx.keys())]]=[list(wx.values())]\n",
    "    \n",
    "    #subselection of k centroids from C, using local Lloyds algorithm with k-means++ initialization\n",
    "    if C.shape[0]<=k:\n",
    "        C_init=C\n",
    "    else:\n",
    "        C_init=localLloyds(C, k, weights=weights, n_iterations=100)\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logParallelInit is not None:\n",
    "        logParallelInit[\"tSamples\"] = tSamples\n",
    "        logParallelInit[\"tCentroids\"] = tCentroids\n",
    "        logParallelInit[\"CostInit\"] = CostInits\n",
    "        logParallelInit[\"tTotal\"] = tEnd - t0\n",
    "\n",
    "    Rdd.unpersist() ###\n",
    "    return C_init\n",
    "\n",
    "def predictedCentroidsLabeler(C_expected, C_predicted):\n",
    "    distMatrix=np.sum((C_expected[:,:,np.newaxis]-C_predicted.T[np.newaxis, :,:])**2,axis=1)\n",
    "    #the labeler i-th entry j, tells that i-th centroid of C_expected is associated to j-th element of C_predicted\n",
    "    labeler=np.argmin(distMatrix,axis=1)\n",
    "    #square distance of element of C_expected to nearest point in C_predicted\n",
    "    distances=distMatrix[np.arange(len(distMatrix)),labeler]\n",
    "    return labeler, distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "741807da-d71c-4192-973b-3207e1d23b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 2 number of partition started at time 1720257774.5732322\n",
      "Finished the pre-steps after 0.5204601287841797 seconds\n",
      "Finished the initialization after 3.766989231109619 seconds\n",
      "The iteration with 2 number of partition ended at time 1720257784.846027 after 10.272794723510742 seconds\n",
      "Starting the naive inizialization part\n",
      "The iteration with 2 number of partition started at time 1720257784.8475716\n",
      "Finished the pre-steps after 0.4569826126098633 seconds\n",
      "Finished the initialization after 0.25000572204589844 seconds\n",
      "The iteration with 2 number of partition ended at time 1720257790.8504238 after 6.002852201461792 seconds\n"
     ]
    }
   ],
   "source": [
    "#### SET UP SPARK ####\n",
    "\n",
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "log4j_conf_path = \"file:///home/quivigorelli/Distributed-K-Means-Clustering/spark/DistributedKmeans/log4j.properties\"\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My first spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"7g\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Dlog4j.configuration=file:{log4j_conf_path}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Eventually clear old data (if re-running)\n",
    "spark.catalog.clearCache() \n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    rdd.unpersist()\n",
    "\n",
    "#### IMPORT THE DATA SET ####\n",
    "\n",
    "data = fetch_kddcup99(return_X_y = True, percent10 = True) # default percent10=True\n",
    "\n",
    "# collect samples and features (target)\n",
    "x = data[0]\n",
    "y = data[1] \n",
    "\n",
    "# cut the data fro memory reasons\n",
    "subLen = 1000\n",
    "x = x[:subLen,]\n",
    "y = y[:subLen]\n",
    "\n",
    "#### PARALLEL\n",
    "\n",
    "# setting up the output information\n",
    "totalLogParallelInit = {}\n",
    "totalLogParallelKmeans = {}\n",
    "tDurationsParallel = {}\n",
    "tPreOperationsParallel = {}\n",
    "\n",
    "# cycle over num_slices to be run\n",
    "#nSlices = [2, 4, 8, 16, 32, 64, 128]\n",
    "nSlices = [2]\n",
    "\n",
    "for nSlice in nSlices:\n",
    "\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # setting up the input and output information for the alghoritm\n",
    "    logParallelInit = {}\n",
    "    logParallelKmeans = {}\n",
    "\n",
    "    k=kTrue\n",
    "    l=k*2 # rescaling probability to have more centroids than k\n",
    "\n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans //\n",
    "    C_init = parallelInit(Rdd, k, l, logParallelInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, 15, logParallelKmeans)\n",
    "    \n",
    "    # time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # output in the correct memory adresses\n",
    "    totalLogParallelInit[f\"Number of partition\" + str(nSlice)] = logParallelInit\n",
    "    totalLogParallelKmeans[f\"Number of partition\" + str(nSlice)] = logParallelKmeans\n",
    "    tDurationsParallel[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsParallel[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    Rdd.unpersist()\n",
    "\n",
    "#### TOTAL OUTPUT ON FILE ####\n",
    "\n",
    "# compute the total log \n",
    "logParallel = {\"totalLogParallelInit\": totalLogParallelInit, \"totalLogParallelKmeans\": totalLogParallelKmeans, \"tDurationsParallel\": tDurationsParallel, \"tPreOperationsParallel\": tPreOperationsParallel}\n",
    "\n",
    "# save the log file\n",
    "if not os.path.exists('dataP'): # create a directory if it doesnt exist\n",
    "    os.makedirs('dataP')\n",
    "\n",
    "with open(pickle_fileP, \"wb\") as file:\n",
    "    pickle.dump(logParallel, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting the naive inizialization part\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### NAIVE RANDOM\n",
    "\n",
    "# setting up some dictionaries\n",
    "totalLogNaiveInit = {}\n",
    "totalLogNaiveKmeans = {}\n",
    "tDurationsNaive = {}\n",
    "tPreOperationsNaive = {}\n",
    "\n",
    "# cycle over num_slices to be run\n",
    "#nSlices = [2, 4, 8, 16, 32, 64, 128]\n",
    "nSlices = [2]\n",
    "\n",
    "for nSlice in nSlices:\n",
    "\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # setting up the input and output information for the alghoritm\n",
    "    logNaiveInit = {}\n",
    "    logNaiveKmeans = {}\n",
    "\n",
    "    k=kTrue\n",
    "    l=k*2 # rescaling probability to have more centroids than k\n",
    "\n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans //\n",
    "    C_init = naiveInitFromSet(Rdd, k, logNaiveInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, 15, logNaiveKmeans)\n",
    "    \n",
    "    # time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # output in the correct memory adresses\n",
    "    totalLogNaiveInit[f\"Number of partition\" + str(nSlice)] = logNaiveInit\n",
    "    totalLogNaiveKmeans[f\"Number of partition\" + str(nSlice)] = logNaiveKmeans\n",
    "    tDurationsNaive[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsNaive[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    Rdd.unpersist()\n",
    "\n",
    "#### TOTAL OUTPUT ON FILE ####\n",
    "\n",
    "# compute the total log \n",
    "logNaive = {\"totalLogNaiveInit\": totalLogNaiveInit, \"totalLogNaiveKmeans\": totalLogNaiveKmeans, \"tDurationsNaive\": tDurationsNaive, \"tPreOperationsNaive\": tPreOperationsNaive}\n",
    "\n",
    "# save the log file\n",
    "if not os.path.exists('dataR'): # create a directory if it doesnt exist\n",
    "    os.makedirs('dataR')\n",
    "\n",
    "with open(pickle_fileR, \"wb\") as filer:\n",
    "    pickle.dump(logNaive, filer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6cb7aa-391b-4378-8e4f-855ce1e6a8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(None,\n",
       "  {'x': array([0.0, 2.6104176374007026e-07, 0.0010571300219495107, 0.0, 0.0, 0.0,\n",
       "          0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "          0.015655577299412915, 0.015655577299412915, 0.0, 0.0, 0.0, 0.0,\n",
       "          1.0, 0.0, 0.0, 0.03529411764705882, 0.03529411764705882, 1.0, 0.0,\n",
       "          0.11, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=object),\n",
       "   'y': b'normal.',\n",
       "   'd2': None})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Rdd.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
