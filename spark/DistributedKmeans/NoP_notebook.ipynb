{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2848fd43-970e-40d6-bdd8-864a29316676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741807da-d71c-4192-973b-3207e1d23b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/05 16:37:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 2 number of partition started at time 1720197440.210167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 3.6871328353881836 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 13.334545135498047 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 2 number of partition ended at time 1720197470.9433377 after 30.733170747756958 seconds\n",
      "The iteration with 4 number of partition started at time 1720197470.9434266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 2.491300106048584 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 16.5985848903656 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 4 number of partition ended at time 1720197509.9558277 after 39.01240110397339 seconds\n",
      "The iteration with 8 number of partition started at time 1720197509.9559097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 2.1317105293273926 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 16.86710810661316 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 8 number of partition ended at time 1720197556.8472636 after 46.89135384559631 seconds\n",
      "The iteration with 16 number of partition started at time 1720197556.8473635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 4.035658597946167 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 25.0942223072052 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 16:40:34 WARN TaskSetManager: Stage 274 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 16 number of partition ended at time 1720197634.0033534 after 77.1559898853302 seconds\n",
      "Starting the naive inizialization part\n",
      "The iteration with 1 number of partition started at time 1720197634.0124164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 16:40:34 WARN TaskSetManager: Stage 276 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:34 WARN TaskSetManager: Stage 277 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 1.0307097434997559 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 16:40:35 WARN TaskSetManager: Stage 278 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:35 WARN TaskSetManager: Stage 279 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:35 WARN TaskSetManager: Stage 280 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 0.47397732734680176 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 16:40:36 WARN TaskSetManager: Stage 282 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:36 WARN TaskSetManager: Stage 283 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:37 WARN TaskSetManager: Stage 285 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:37 WARN TaskSetManager: Stage 286 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:37 WARN TaskSetManager: Stage 288 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:38 WARN TaskSetManager: Stage 289 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:38 WARN TaskSetManager: Stage 291 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:39 WARN TaskSetManager: Stage 292 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:39 WARN TaskSetManager: Stage 294 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:39 WARN TaskSetManager: Stage 295 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:40 WARN TaskSetManager: Stage 297 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:40 WARN TaskSetManager: Stage 298 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:41 WARN TaskSetManager: Stage 300 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:41 WARN TaskSetManager: Stage 301 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:42 WARN TaskSetManager: Stage 303 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:42 WARN TaskSetManager: Stage 304 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:43 WARN TaskSetManager: Stage 306 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:43 WARN TaskSetManager: Stage 307 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:43 WARN TaskSetManager: Stage 309 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:44 WARN TaskSetManager: Stage 310 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:44 WARN TaskSetManager: Stage 312 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:44 WARN TaskSetManager: Stage 313 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:45 WARN TaskSetManager: Stage 315 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:45 WARN TaskSetManager: Stage 316 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:46 WARN TaskSetManager: Stage 318 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:46 WARN TaskSetManager: Stage 319 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:47 WARN TaskSetManager: Stage 321 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:47 WARN TaskSetManager: Stage 322 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 16:40:48 WARN TaskSetManager: Stage 324 contains a task of very large size (1255 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 1 number of partition ended at time 1720197648.1631744 after 14.150758028030396 seconds\n",
      "The iteration with 2 number of partition started at time 1720197648.1632445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 1.0390682220458984 seconds\n",
      "Finished the initialization after 0.5185818672180176 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 2 number of partition ended at time 1720197662.127421 after 13.964176416397095 seconds\n"
     ]
    }
   ],
   "source": [
    "#### IMPORT LIBRARIES #### \n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from functions import *\n",
    "from time import time\n",
    "\n",
    "timeString=str(time())\n",
    "#### SELECT THE PICKLE FILE ####\n",
    "pickle_fileP = 'dataP/log1P'+'_'+timeString+'.pkl'\n",
    "pickle_fileR = 'dataR/log1U'+'_'+timeString+'.pkl'\n",
    "\n",
    "###### FUNCTIONS #######\n",
    "###### FUNCTIONS #######\n",
    "def labelToInt(label):\n",
    "    \"\"\"\n",
    "    Let's define a map from Y (set of strings) into (0,size(Y)) for easier usage\n",
    "    \"\"\"\n",
    "    uniqueLabels=list(np.unique(y))\n",
    "    return uniqueLabels.index(label)\n",
    "\n",
    "\n",
    "def deleteBytes(datum):\n",
    "    x = datum[1][\"x\"]\n",
    "    mask = [type(i) != bytes for i in x]\n",
    "    datum[1][\"x\"] = np.asarray(x[mask])\n",
    "    print(x)\n",
    "    print(mask)\n",
    "    return datum\n",
    "    \n",
    "def localPlusPlusInit(points, k): \n",
    "    #print('pointsshape: ', points.shape)\n",
    "    '''\n",
    "    Initialization kmeans ++\n",
    "    points is a numpy array (n,dim)\n",
    "    '''\n",
    "    C=points[np.random.choice(points.shape[0])]#sample from array di punti ecc...\n",
    "    C=C[np.newaxis, :]\n",
    "    for _ in range(k):\n",
    "        #points is array (n, dim), C is array(g<=k, dim)\n",
    "        #probs is array (n,1)\n",
    "        probs=np.min(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1).flatten()\n",
    "        #probs=[min([sum((point-centroid)**2) for centroid in C]) for point in points] #numpyfy this, or numbafy if left base python\n",
    "        probs=probs/np.sum(probs)\n",
    "        nextCentroid=points[np.random.choice(points.shape[0], p=probs)][np.newaxis,:]\n",
    "        #print('LE FORME',C.shape, nextCentroid.shape)\n",
    "        C=np.vstack((C, nextCentroid))\n",
    "    return C\n",
    "\n",
    "\n",
    "def weightedAverage(group):\n",
    "    \"\"\"\n",
    "    Function to compute the weighted average\n",
    "    \"\"\"\n",
    "    weight_column='weights'\n",
    "    groupby_column='clusterId'\n",
    "    columns_to_average = group.columns.difference([weight_column, groupby_column])\n",
    "    weighted_averages = group[columns_to_average].multiply(group[weight_column], axis=0).sum() / group[weight_column].sum()\n",
    "    return weighted_averages\n",
    "\n",
    "\n",
    "def localLloyds(points, k, weights=None, n_iterations=100):\n",
    "    \"\"\"\n",
    "    function that does the Local Lloyds algorithm\n",
    "    \"\"\"\n",
    "    df=pd.DataFrame(points)\n",
    "    if weights is None:\n",
    "        weights=np.ones(shape=len(points))\n",
    "    #print('weights', weights)\n",
    "    df['weights']=weights\n",
    "    df['clusterId']=np.zeros(shape=len(points))\n",
    "    C=localPlusPlusInit(points, k)\n",
    "    #print('localPlusPluisInit: ', C)\n",
    "    clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "    for iteration in range(n_iterations):\n",
    "        df['clusterId']=clusterId\n",
    "        C_df=df.groupby('clusterId')\\\n",
    "            .apply(weightedAverage)\\\n",
    "            .reset_index()\n",
    "        C_array=C_df[C_df.columns.difference(['weights', 'clusterId'])].reset_index(drop=True).to_numpy()\n",
    "        clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C_array.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "        #print(clusterId)\n",
    "        \n",
    "    return C_array   \n",
    "\n",
    "\n",
    "def minmaxRescale(datum, minS, maxS):\n",
    "    \"\"\"\n",
    "    Rescale a datum in [0,1]\n",
    "    \"\"\"\n",
    "    mask = (minS < maxS).astype(bool)\n",
    "    feature = datum[1][\"x\"] \n",
    "    feature = (feature[mask] - minS[mask])/(maxS[mask] - minS[mask])\n",
    "    return (datum[0], {\"x\": feature, \"y\": datum[1][\"y\"], \"d2\":datum[1][\"d2\"]}) \n",
    "\n",
    "\n",
    "def selectCluster(datum, C, updateDistances=True):\n",
    "    \"\"\"\n",
    "    Associates a datum to its centroid and updates the distance if True\n",
    "    dimC(k, len(datum))\n",
    "    \"\"\"\n",
    "    distances = np.sum((datum[1][\"x\"] - C)**2, axis=1)\n",
    "    print('distances: ',distances)\n",
    "    clusterId = np.argmin(distances)\n",
    "    if updateDistances is True:\n",
    "        return (clusterId, {'x':datum[1]['x'], 'y':datum[1]['y'], 'd2':distances[clusterId]})\n",
    "    else:\n",
    "        return (clusterId, datum[1])\n",
    "\n",
    "\n",
    "def updateCentroids(Rdd):\n",
    "    \"\"\"\n",
    "    update centroids as 'centers of mass' of clusters\n",
    "    \"\"\"\n",
    "    C=Rdd.mapValues(lambda xy: (xy['x'], 1))\\\n",
    "              .reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1]))\\\n",
    "              .mapValues(lambda a:a[0]/a[1])\\\n",
    "              .values()\\\n",
    "              .collect() \n",
    "    C=np.array(C) #check later more carefully if causes some overhead\n",
    "    return C\n",
    "\n",
    "\n",
    "def updateDistances(Rdd, C):\n",
    "    \"\"\"\n",
    "    update the Rdd with square distances from centroids, given Rdd with centroids already updated\n",
    "    \"\"\"\n",
    "    def datumUpdate(datum, C):\n",
    "        d2=np.sum((datum[1]['x']-C[datum[0]])**2)\n",
    "        #return datum\n",
    "        return (datum[0], {\"x\": datum[1][\"x\"], \"y\": datum[1][\"y\"], \"d2\":d2})\n",
    "    Rdd=Rdd.map(lambda datum:datumUpdate(datum, C))\n",
    "    return Rdd\n",
    "\n",
    "\n",
    "def cost(Rdd):\n",
    "    \"\"\"\n",
    "    calculate global cost of X,C from an Rdd with distances from centroids already updated\n",
    "    \"\"\"\n",
    "    my_cost=Rdd.map(lambda datum : datum[1]['d2'])\\\n",
    "               .reduce(lambda a,b: a+b)\n",
    "    return my_cost \n",
    "\n",
    "\n",
    "def kMeans(Rdd, C_init, maxIterations, logParallelKmeans=None):\n",
    "    \"\"\"\n",
    "    kMeans in parallel (?)\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    my_kMeansCosts = []\n",
    "    tIterations = []\n",
    "    C=C_init\n",
    "\n",
    "    for t in range(maxIterations):\n",
    "        t1 = time()\n",
    "        RddCached = Rdd.map(lambda datum: selectCluster(datum, C)).persist()\n",
    "        # Now we compute the new centroids by calculating the averages of points belonging to the same cluster.\n",
    "        # Need to check that all centroids are assigned to at least one point, otherwise k changes!!! Solutions?!\n",
    "        C=updateCentroids(RddCached)\n",
    "        my_cost = cost(RddCached)\n",
    "        \n",
    "        my_kMeansCosts.append(my_cost)\n",
    "        t2 = time()\n",
    "        \n",
    "        tIteration = t2 - t1\n",
    "        tIterations.append(tIteration)\n",
    "        \n",
    "        #RddCached.unpersist() bad for time efficiency, not necessary due to Python Garbage collector\n",
    "        \n",
    "\n",
    "    tEnd = time()\n",
    "    tTotal = tEnd - t0\n",
    "    \n",
    "    if logParallelKmeans is not None:\n",
    "        logParallelKmeans[\"CostsKmeans\"] = my_kMeansCosts\n",
    "        logParallelKmeans[\"tIterations\"] = tIterations\n",
    "        logParallelKmeans[\"tTotal\"] = tTotal\n",
    "        \n",
    "    return C\n",
    "\n",
    "\n",
    "def naiveInitFromSet(Rdd, k, logNaiveInit=None):\n",
    "    \"\"\"\n",
    "    uniform sampling of k points from Rdd\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    kSubset=Rdd.takeSample(False, k)\n",
    "    # Replacement is set to False to avoid coinciding centroids BUT no guarantees that in the original dataset all points are distinct!!! Check if causes problems in the algorithm (i.e. need to pre-filter) or it's ok\n",
    "    C_init=np.array([datum[1]['x'] for datum in kSubset])\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logNaiveInit is not None:\n",
    "        logNaiveInit[\"tTotal\"] = tEnd - t0\n",
    "        \n",
    "    return C_init\n",
    "\n",
    "\n",
    "def naiveInitFromSpace(k, dim):\n",
    "    \"\"\"\n",
    "    #uniform drawing of k points from euclidean space\n",
    "    #we assume the Rdd has been mapped into a [0,1]^dim space\n",
    "    \"\"\"\n",
    "    C_init=np.random.uniform(size=(k,dim))\n",
    "    return C_init\n",
    "\n",
    "\n",
    "def parallelInit(Rdd, k, l, logParallelInit=None):\n",
    "    \"\"\"\n",
    "    Parallel initialization\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    # initialize C as a point in the dataset\n",
    "    C=naiveInitFromSet(Rdd, 1) \n",
    "    \n",
    "    # associate each datum to the only centroid (computed before) and computed distances and cost\n",
    "    Rdd=Rdd.map(lambda datum : (0, datum[1]))\n",
    "    Rdd=updateDistances(Rdd, C).persist()\n",
    "    my_cost=cost(Rdd)\n",
    "\n",
    "    # number of iterations (log(cost))\n",
    "    n_iterations=int(np.log(my_cost))\n",
    "    if(n_iterations<1): n_iterations=1\n",
    "\n",
    "    \n",
    "    tSamples = []\n",
    "    tCentroids = []\n",
    "    CostInits = [my_cost]\n",
    "    # iterative sampling of the centroids\n",
    "    for _ in range(n_iterations):\n",
    "\n",
    "        t1=time()\n",
    "        # sample C' according to the probability\n",
    "        C_prime=Rdd.filter(lambda datum : np.random.uniform()<l*datum[1]['d2']/my_cost)\\\n",
    "                   .map(lambda datum : datum[1]['x'])\\\n",
    "                   .collect()\n",
    "        C_prime=np.array(C_prime)\n",
    "        t2=time()\n",
    "\n",
    "        # stack C and C', update distances, centroids, and cost\n",
    "        if (C_prime.shape[0]>0):\n",
    "            C=np.vstack((C, C_prime))\n",
    "            Rdd=Rdd.map(lambda datum: selectCluster(datum, C)).persist()\n",
    "            my_cost=cost(Rdd)\n",
    "        t3=time()\n",
    "\n",
    "        tSample = t2 -t1\n",
    "        tCentroid = t3 - t2\n",
    "        tSamples.append(tSample)\n",
    "        tCentroids.append(tCentroid)\n",
    "        CostInits.append(my_cost)\n",
    "       \n",
    "    #erase centroids sampled more than once \n",
    "    C=C.astype(float)\n",
    "    C=np.unique(C, axis=0)\n",
    "    Rdd=Rdd.map(lambda datum: selectCluster(datum, C))\n",
    "    \n",
    "    #compute weights of centroids (sizes of each cluster) and put them in a list whose index is same centroid index as C\n",
    "    wx=Rdd.countByKey()\n",
    "    weights=np.zeros(len(C))\n",
    "    weights[[list(wx.keys())]]=[list(wx.values())]\n",
    "    \n",
    "    #subselection of k centroids from C, using local Lloyds algorithm with k-means++ initialization\n",
    "    if C.shape[0]<=k:\n",
    "        C_init=C\n",
    "    else:\n",
    "        C_init=localLloyds(C, k, weights=weights, n_iterations=100)\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logParallelInit is not None:\n",
    "        logParallelInit[\"tSamples\"] = tSamples\n",
    "        logParallelInit[\"tCentroids\"] = tCentroids\n",
    "        logParallelInit[\"CostInit\"] = CostInits\n",
    "        logParallelInit[\"tTotal\"] = tEnd - t0\n",
    "        \n",
    "    return C_init\n",
    "\n",
    "def predictedCentroidsLabeler(C_expected, C_predicted):\n",
    "    distMatrix=np.sum((C_expected[:,:,np.newaxis]-C_predicted.T[np.newaxis, :,:])**2,axis=1)\n",
    "    #the labeler i-th entry j, tells that i-th centroid of C_expected is associated to j-th element of C_predicted\n",
    "    labeler=np.argmin(distMatrix,axis=1)\n",
    "    #square distance of element of C_expected to nearest point in C_predicted\n",
    "    distances=distMatrix[np.arange(len(distMatrix)),labeler]\n",
    "    return labeler, distances\n",
    "\n",
    "\n",
    "\n",
    "#### SET UP SPARK ####\n",
    "\n",
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My first spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "#### IMPORT THE DATA SET ####\n",
    "\n",
    "data = fetch_kddcup99(return_X_y = True, percent10 = True) # default percent10=True\n",
    "\n",
    "# collect samples and features (target)\n",
    "x = data[0]\n",
    "y = data[1] \n",
    "\n",
    "# cut the data fro memory reasons\n",
    "subLen = 5000\n",
    "x = x[:subLen,]\n",
    "y = y[:subLen]\n",
    "\n",
    "#### PARALLEL\n",
    "\n",
    "# setting up the output information\n",
    "totalLogParallelInit = {}\n",
    "totalLogParallelKmeans = {}\n",
    "tDurationsParallel = {}\n",
    "tPreOperationsParallel = {}\n",
    "\n",
    "# cycle over num_slices to be run\n",
    "nSlices = [2, 4, 8, 16]\n",
    "#nSlices = [1, 2]\n",
    "\n",
    "for nSlice in nSlices:\n",
    "\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # setting up the input and output information for the alghoritm\n",
    "    logParallelInit = {}\n",
    "    logParallelKmeans = {}\n",
    "\n",
    "    k=kTrue\n",
    "    l=k*2 # rescaling probability to have more centroids than k\n",
    "\n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans //\n",
    "    C_init = parallelInit(Rdd, k, l, logParallelInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, 15, logParallelKmeans)\n",
    "    \n",
    "    # time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # output in the correct memory adresses\n",
    "    totalLogParallelInit[f\"Number of partition\" + str(nSlice)] = logParallelInit\n",
    "    totalLogParallelKmeans[f\"Number of partition\" + str(nSlice)] = logParallelKmeans\n",
    "    tDurationsParallel[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsParallel[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "#### TOTAL OUTPUT ON FILE ####\n",
    "\n",
    "# compute the total log \n",
    "logParallel = {\"totalLogParallelInit\": totalLogParallelInit, \"totalLogParallelKmeans\": totalLogParallelKmeans, \"tDurationsParallel\": tDurationsParallel, \"tPreOperationsParallel\": tPreOperationsParallel}\n",
    "\n",
    "# save the log file\n",
    "if not os.path.exists('dataP'): # create a directory if it doesnt exist\n",
    "    os.makedirs('dataP')\n",
    "\n",
    "with open(pickle_fileP, \"wb\") as file:\n",
    "    pickle.dump(logParallel, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting the naive inizialization part\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### NAIVE RANDOM\n",
    "\n",
    "# setting up some dictionaries\n",
    "totalLogNaiveInit = {}\n",
    "totalLogNaiveKmeans = {}\n",
    "tDurationsNaive = {}\n",
    "tPreOperationsNaive = {}\n",
    "\n",
    "# cycle over num_slices to be run\n",
    "# nSlices = [2, 4, 8, 16, 32, 64]\n",
    "nSlices = [1, 2]\n",
    "\n",
    "for nSlice in nSlices:\n",
    "\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # setting up the input and output information for the alghoritm\n",
    "    logNaiveInit = {}\n",
    "    logNaiveKmeans = {}\n",
    "\n",
    "    k=kTrue\n",
    "    l=k*2 # rescaling probability to have more centroids than k\n",
    "\n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans //\n",
    "    C_init = naiveInitFromSet(Rdd, k, logNaiveInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, 15, logNaiveKmeans)\n",
    "    \n",
    "    # time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # output in the correct memory adresses\n",
    "    totalLogNaiveInit[f\"Number of partition\" + str(nSlice)] = logNaiveInit\n",
    "    totalLogNaiveKmeans[f\"Number of partition\" + str(nSlice)] = logNaiveKmeans\n",
    "    tDurationsNaive[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsNaive[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "#### TOTAL OUTPUT ON FILE ####\n",
    "\n",
    "# compute the total log \n",
    "logNaive = {\"totalLogNaiveInit\": totalLogNaiveInit, \"totalLogNaiveKmeans\": totalLogNaiveKmeans, \"tDurationsNaive\": tDurationsNaive, \"tPreOperationsNaive\": tPreOperationsNaive}\n",
    "\n",
    "# save the log file\n",
    "if not os.path.exists('dataR'): # create a directory if it doesnt exist\n",
    "    os.makedirs('dataR')\n",
    "\n",
    "with open(pickle_fileR, \"wb\") as filer:\n",
    "    pickle.dump(logNaive, filer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cb7aa-391b-4378-8e4f-855ce1e6a8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
