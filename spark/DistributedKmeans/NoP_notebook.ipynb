{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44221217-17fd-418a-9ce2-bead05f58c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e840b600-ca65-4950-952d-1fade8c9614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356820d8-58d4-4c05-9eae-0f714778d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "logging.getLogger('pyspark').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2848fd43-970e-40d6-bdd8-864a29316676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b7eed2-1d38-40a6-825a-d9fc201f29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORT LIBRARIES #### \n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from functions import *\n",
    "from time import time\n",
    "import subprocess\n",
    "\n",
    "np.random.seed(12345)\n",
    "spark_seed = 54321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826cb1c4-f323-43b0-a155-f6dec45e08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SELECT THE PICKLE FILE ####\n",
    "timeString=str(time())\n",
    "pickle_fileP = 'dataP/log1P_1.pkl'\n",
    "pickle_fileR = 'dataR/log1U_1.pkl'\n",
    "\n",
    "if os.path.isfile(pickle_fileP):\n",
    "    with open(pickle_fileP, \"rb\") as f:\n",
    "        logParallel = pickle.load(f)\n",
    "        totalLogParallelInit, totalLogParallelKmeans, tDurationsParallel, tPreOperationsParallel = logParallel.values()\n",
    "else:\n",
    "    totalLogParallelInit = {}\n",
    "    totalLogParallelKmeans = {}\n",
    "    tDurationsParallel = {}\n",
    "    tPreOperationsParallel = {}\n",
    "\n",
    "\n",
    "if os.path.isfile(pickle_fileR):\n",
    "    with open(pickle_fileR, \"rb\") as f:\n",
    "        logNaive = pickle.load(f)\n",
    "        totalLogNaiveInit, totalLogNaiveKmeans, tDurationsNaive, tPreOperationsNaive = logNaive.values()\n",
    "else:\n",
    "    totalLogNaiveInit = {}\n",
    "    totalLogNaiveKmeans = {}\n",
    "    tDurationsNaive = {}\n",
    "    tPreOperationsNaive = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df5cd7f-8a8a-49f4-b044-e046a53d0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle over num_slices to be run\n",
    "nSlices = [8]\n",
    "subLen = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2bb6219-6d89-4b2e-a565-58b3868931d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './*': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=\"ssh slave3 'cd /usr/local/spark/work/; rm -r ./*'\", returncode=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\"ssh slave2 'cd /usr/local/spark/work/; rm -r ./*'\", shell=True)\n",
    "subprocess.run(\"ssh slave3 'cd /usr/local/spark/work/; rm -r ./*'\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1426166-735a-4146-9df4-3bf35d3e6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FUNCTIONS #######\n",
    "def labelToInt(label):\n",
    "    \"\"\"\n",
    "    Let's define a map from Y (set of strings) into (0,size(Y)) for easier usage\n",
    "    \"\"\"\n",
    "    uniqueLabels=list(np.unique(y))\n",
    "    return uniqueLabels.index(label)\n",
    "\n",
    "\n",
    "def deleteBytes(datum):\n",
    "    x = datum[1][\"x\"]\n",
    "    mask = [type(i) != bytes for i in x]\n",
    "    datum[1][\"x\"] = np.asarray(x[mask])\n",
    "    print(x)\n",
    "    print(mask)\n",
    "    return datum\n",
    "    \n",
    "def localPlusPlusInit(points, k): \n",
    "    #print('pointsshape: ', points.shape)\n",
    "    '''\n",
    "    Initialization kmeans ++\n",
    "    points is a numpy array (n,dim)\n",
    "    '''\n",
    "    C=points[np.random.choice(points.shape[0])]#sample from array di punti ecc...\n",
    "    C=C[np.newaxis, :]\n",
    "    for _ in range(k):\n",
    "        #points is array (n, dim), C is array(g<=k, dim)\n",
    "        #probs is array (n,1)\n",
    "        probs=np.min(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1).flatten()\n",
    "        #probs=[min([sum((point-centroid)**2) for centroid in C]) for point in points] #numpyfy this, or numbafy if left base python\n",
    "        probs=probs/np.sum(probs)\n",
    "        nextCentroid=points[np.random.choice(points.shape[0], p=probs)][np.newaxis,:]\n",
    "        #print('LE FORME',C.shape, nextCentroid.shape)\n",
    "        C=np.vstack((C, nextCentroid))\n",
    "    return C\n",
    "\n",
    "\n",
    "def weightedAverage(group):\n",
    "    \"\"\"\n",
    "    Function to compute the weighted average\n",
    "    \"\"\"\n",
    "    weight_column='weights'\n",
    "    groupby_column='clusterId'\n",
    "    columns_to_average = group.columns.difference([weight_column, groupby_column])\n",
    "    weighted_averages = group[columns_to_average].multiply(group[weight_column], axis=0).sum() / group[weight_column].sum()\n",
    "    return weighted_averages\n",
    "\n",
    "\n",
    "def localLloyds(points, k, weights=None, n_iterations=100):\n",
    "    \"\"\"\n",
    "    function that does the Local Lloyds algorithm\n",
    "    \"\"\"\n",
    "    df=pd.DataFrame(points)\n",
    "    if weights is None:\n",
    "        weights=np.ones(shape=len(points))\n",
    "    #print('weights', weights)\n",
    "    df['weights']=weights\n",
    "    df['clusterId']=np.zeros(shape=len(points))\n",
    "    C=localPlusPlusInit(points, k)\n",
    "    #print('localPlusPluisInit: ', C)\n",
    "    clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "    for iteration in range(n_iterations):\n",
    "        df['clusterId']=clusterId\n",
    "        C_df=df.groupby('clusterId')\\\n",
    "            .apply(weightedAverage)\\\n",
    "            .reset_index()\n",
    "        C_array=C_df[C_df.columns.difference(['weights', 'clusterId'])].reset_index(drop=True).to_numpy()\n",
    "        clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C_array.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "        #print(clusterId)\n",
    "\n",
    "    return C_array   \n",
    "\n",
    "\n",
    "def minmaxRescale(datum, minS, maxS):\n",
    "    \"\"\"\n",
    "    Rescale a datum in [0,1]\n",
    "    \"\"\"\n",
    "    mask = (minS < maxS).astype(bool)\n",
    "    feature = datum[1][\"x\"] \n",
    "    feature = (feature[mask] - minS[mask])/(maxS[mask] - minS[mask])\n",
    "    return (datum[0], {\"x\": feature, \"y\": datum[1][\"y\"], \"d2\":datum[1][\"d2\"]}) \n",
    "\n",
    "\n",
    "def selectCluster(datum, C, updateDistances=True):\n",
    "    \"\"\"\n",
    "    Associates a datum to its centroid and updates the distance if True\n",
    "    dimC(k, len(datum))\n",
    "    \"\"\"\n",
    "    distances = np.sum((datum[1][\"x\"] - C)**2, axis=1)\n",
    "    print('distances: ',distances)\n",
    "    clusterId = np.argmin(distances)\n",
    "    if updateDistances is True:\n",
    "        return (clusterId, {'x':datum[1]['x'], 'y':datum[1]['y'], 'd2':distances[clusterId]})\n",
    "    else:\n",
    "        return (clusterId, datum[1])\n",
    "\n",
    "\n",
    "def updateCentroids(Rdd):\n",
    "    \"\"\"\n",
    "    update centroids as 'centers of mass' of clusters\n",
    "    \"\"\"\n",
    "    C=Rdd.mapValues(lambda xy: (xy['x'], 1))\\\n",
    "              .reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1]))\\\n",
    "              .mapValues(lambda a:a[0]/a[1])\\\n",
    "              .values()\\\n",
    "              .collect() \n",
    "    C=np.array(C) #check later more carefully if causes some overhead\n",
    "    return C\n",
    "\n",
    "\n",
    "def updateDistances(Rdd, C):\n",
    "    \"\"\"\n",
    "    update the Rdd with square distances from centroids, given Rdd with centroids already updated\n",
    "    \"\"\"\n",
    "    def datumUpdate(datum, C):\n",
    "        d2=np.sum((datum[1]['x']-C[datum[0]])**2)\n",
    "        #return datum\n",
    "        return (datum[0], {\"x\": datum[1][\"x\"], \"y\": datum[1][\"y\"], \"d2\":d2})\n",
    "    Rdd=Rdd.map(lambda datum:datumUpdate(datum, C))\n",
    "    return Rdd\n",
    "\n",
    "\n",
    "def cost(Rdd):\n",
    "    \"\"\"\n",
    "    calculate global cost of X,C from an Rdd with distances from centroids already updated\n",
    "    \"\"\"\n",
    "    my_cost=Rdd.map(lambda datum : datum[1]['d2'])\\\n",
    "               .reduce(lambda a,b: a+b)\n",
    "    return my_cost \n",
    "\n",
    "\n",
    "def kMeans(Rdd, C_init, maxIterations, logParallelKmeans=None):\n",
    "    \"\"\"\n",
    "    kMeans in parallel (?)\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    my_kMeansCosts = []\n",
    "    tIterations = []\n",
    "    C=C_init\n",
    "\n",
    "    for t in range(maxIterations):\n",
    "        t1 = time()\n",
    "        RddCached = Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "        \n",
    "        # Now we compute the new centroids by calculating the averages of points belonging to the same cluster.\n",
    "        # Need to check that all centroids are assigned to at least one point, otherwise k changes!!! Solutions?!\n",
    "        C=updateCentroids(RddCached)\n",
    "        my_cost = cost(RddCached)\n",
    "        \n",
    "        my_kMeansCosts.append(my_cost)\n",
    "        t2 = time()\n",
    "        \n",
    "        tIteration = t2 - t1\n",
    "        tIterations.append(tIteration)\n",
    "        \n",
    "        RddCached.unpersist() ### bad for time efficiency, not necessary due to Python Garbage collector (?)\n",
    "\n",
    "        if (len(my_kMeansCosts) > 1) and (my_kMeansCosts[-1] > 0.99*my_kMeansCosts[-2]):\n",
    "            break\n",
    "\n",
    "    tEnd = time()\n",
    "    tTotal = tEnd - t0\n",
    "    \n",
    "    if logParallelKmeans is not None:\n",
    "        logParallelKmeans[\"CostsKmeans\"] = my_kMeansCosts\n",
    "        logParallelKmeans[\"tIterations\"] = tIterations\n",
    "        logParallelKmeans[\"tTotal\"] = tTotal\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def naiveInitFromSet(Rdd, k, logNaiveInit=None):\n",
    "    \"\"\"\n",
    "    uniform sampling of k points from Rdd\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    kSubset=Rdd.takeSample(False, k, seed=spark_seed)\n",
    "    # Replacement is set to False to avoid coinciding centroids BUT no guarantees that in the original dataset all points are distinct!!! Check if causes problems in the algorithm (i.e. need to pre-filter) or it's ok\n",
    "    C_init=np.array([datum[1]['x'] for datum in kSubset])\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logNaiveInit is not None:\n",
    "        logNaiveInit[\"tTotal\"] = tEnd - t0\n",
    "        \n",
    "    return C_init\n",
    "\n",
    "\n",
    "def naiveInitFromSpace(k, dim):\n",
    "    \"\"\"\n",
    "    #uniform drawing of k points from euclidean space\n",
    "    #we assume the Rdd has been mapped into a [0,1]^dim space\n",
    "    \"\"\"\n",
    "    C_init=np.random.uniform(size=(k,dim))\n",
    "    return C_init\n",
    "\n",
    "\n",
    "def parallelInit(Rdd, k, l, logParallelInit=None):\n",
    "    \"\"\"\n",
    "    Parallel initialization\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    # initialize C as a point in the dataset\n",
    "    C=naiveInitFromSet(Rdd, 1) \n",
    "    \n",
    "    # associate each datum to the only centroid (computed before) and computed distances and cost\n",
    "    Rdd=Rdd.map(lambda datum : (0, datum[1]))\n",
    "    Rdd=updateDistances(Rdd, C).persist() ###\n",
    "    \n",
    "    my_cost=cost(Rdd)\n",
    "\n",
    "    # number of iterations (log(cost))\n",
    "    n_iterations=int(np.log(my_cost))\n",
    "    if(n_iterations<1): n_iterations=1\n",
    "\n",
    "    \n",
    "    tSamples = []\n",
    "    tCentroids = []\n",
    "    CostInits = [my_cost]\n",
    "    # iterative sampling of the centroids\n",
    "    for _ in range(n_iterations):\n",
    "\n",
    "        t1=time()\n",
    "        # sample C' according to the probability\n",
    "        C_prime=Rdd.filter(lambda datum : np.random.uniform()<l*datum[1]['d2']/my_cost)\\\n",
    "                   .map(lambda datum : datum[1]['x'])\\\n",
    "                   .collect()\n",
    "        C_prime=np.array(C_prime)\n",
    "        t2=time()\n",
    "\n",
    "        # stack C and C', update distances, centroids, and cost\n",
    "        if (C_prime.shape[0]>0):\n",
    "            C=np.vstack((C, C_prime))\n",
    "            \n",
    "            Rdd.unpersist() ###\n",
    "            Rdd=Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "            \n",
    "            my_cost=cost(Rdd)\n",
    "        t3=time()\n",
    "\n",
    "        tSample = t2 -t1\n",
    "        tCentroid = t3 - t2\n",
    "        tSamples.append(tSample)\n",
    "        tCentroids.append(tCentroid)\n",
    "        CostInits.append(my_cost)\n",
    "       \n",
    "    #erase centroids sampled more than once \n",
    "    C=C.astype(float)\n",
    "    C=np.unique(C, axis=0)\n",
    "    Rdd=Rdd.map(lambda datum: selectCluster(datum, C))\n",
    "    \n",
    "    #compute weights of centroids (sizes of each cluster) and put them in a list whose index is same centroid index as C\n",
    "    wx=Rdd.countByKey()\n",
    "    weights=np.zeros(len(C))\n",
    "    weights[[list(wx.keys())]]=[list(wx.values())]\n",
    "    \n",
    "    #subselection of k centroids from C, using local Lloyds algorithm with k-means++ initialization\n",
    "    if C.shape[0]<=k:\n",
    "        C_init=C\n",
    "    else:\n",
    "        C_init=localLloyds(C, k, weights=weights, n_iterations=100)\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logParallelInit is not None:\n",
    "        logParallelInit[\"tSamples\"] = tSamples\n",
    "        logParallelInit[\"tCentroids\"] = tCentroids\n",
    "        logParallelInit[\"CostInit\"] = CostInits\n",
    "        logParallelInit[\"tTotal\"] = tEnd - t0\n",
    "\n",
    "    Rdd.unpersist() ###\n",
    "    return C_init\n",
    "\n",
    "def predictedCentroidsLabeler(C_expected, C_predicted):\n",
    "    distMatrix=np.sum((C_expected[:,:,np.newaxis]-C_predicted.T[np.newaxis, :,:])**2,axis=1)\n",
    "    #the labeler i-th entry j, tells that i-th centroid of C_expected is associated to j-th element of C_predicted\n",
    "    labeler=np.argmin(distMatrix,axis=1)\n",
    "    #square distance of element of C_expected to nearest point in C_predicted\n",
    "    distances=distMatrix[np.arange(len(distMatrix)),labeler]\n",
    "    return labeler, distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741807da-d71c-4192-973b-3207e1d23b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/06 15:10:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 8 number of partition started at time 1720278613.299945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 3.4235100746154785 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 489.69173073768616 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 8 number of partition ended at time 1720279110.230998 after 496.9310529232025 seconds\n",
      "Persisted RRDs:  2\n",
      "Persisted RRDs:  0\n",
      "Starting the naive inizialization part\n",
      "The iteration with 8 number of partition started at time 1720279110.2553244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 1.8723645210266113 seconds\n",
      "Finished the initialization after 0.5764014720916748 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 8 number of partition ended at time 1720279125.4293597 after 15.17403531074524 seconds\n",
      "Persisted RRDs:  1\n",
      "Persisted RRDs:  0\n",
      "CPU times: user 3.74 s, sys: 605 ms, total: 4.35 s\n",
      "Wall time: 8min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### SET UP SPARK ####\n",
    "\n",
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "log4j_conf_path = \"file:///home/quivigorelli/Distributed-K-Means-Clustering/spark/DistributedKmeans/log4j.properties\"\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My first spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"7g\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Dlog4j.configuration=file:{log4j_conf_path}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Eventually clear old data (if re-running)\n",
    "spark.catalog.clearCache() \n",
    "for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    rdd.unpersist()\n",
    "\n",
    "#### IMPORT THE DATA SET ####\n",
    "\n",
    "data = fetch_kddcup99(return_X_y = True, percent10 = True) # default percent10=True\n",
    "\n",
    "# collect samples and features (target)\n",
    "x = data[0]\n",
    "y = data[1] \n",
    "\n",
    "# Shuffle\n",
    "shuffled_indices = np.random.permutation(len(x))\n",
    "x=x[shuffled_indices]\n",
    "y=y[shuffled_indices]\n",
    "\n",
    "# cut the data fro memory reasons\n",
    "x = x[:subLen,]\n",
    "y = y[:subLen]\n",
    "\n",
    "#### PARALLEL\n",
    "for nSlice in nSlices:\n",
    "\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # setting up the input and output information for the alghoritm\n",
    "    logParallelInit = {}\n",
    "    logParallelKmeans = {}\n",
    "\n",
    "    k=kTrue\n",
    "    l=k*2 # rescaling probability to have more centroids than k\n",
    "\n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans //\n",
    "    C_init = parallelInit(Rdd, k, l, logParallelInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, 15, logParallelKmeans)\n",
    "    \n",
    "    # time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # output in the correct memory adresses\n",
    "    totalLogParallelInit[f\"Number of partition\" + str(nSlice)] = logParallelInit\n",
    "    totalLogParallelKmeans[f\"Number of partition\" + str(nSlice)] = logParallelKmeans\n",
    "    tDurationsParallel[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsParallel[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "    for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "    print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "\n",
    "#### TOTAL OUTPUT ON FILE ####\n",
    "# compute the total log\n",
    "logParallel = {\"totalLogParallelInit\": totalLogParallelInit, \"totalLogParallelKmeans\": totalLogParallelKmeans, \"tDurationsParallel\": tDurationsParallel, \"tPreOperationsParallel\": tPreOperationsParallel}\n",
    "\n",
    "\n",
    "# save the log file\n",
    "if not os.path.exists('dataP'): # create a directory if it doesnt exist\n",
    "    os.makedirs('dataP')\n",
    "\n",
    "with open(pickle_fileP, \"wb\") as file:\n",
    "    pickle.dump(logParallel, file)\n",
    "\n",
    "\n",
    "print(\"Starting the naive inizialization part\")\n",
    "\n",
    "\n",
    "#### NAIVE RANDOM\n",
    "for nSlice in nSlices:\n",
    "\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # setting up the input and output information for the alghoritm\n",
    "    logNaiveInit = {}\n",
    "    logNaiveKmeans = {}\n",
    "\n",
    "    k=kTrue\n",
    "    l=k*2 # rescaling probability to have more centroids than k\n",
    "\n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans//\n",
    "    C_init = naiveInitFromSet(Rdd, k, logNaiveInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, 15, logNaiveKmeans)\n",
    "    \n",
    "    # time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # output in the correct memory adresses\n",
    "    totalLogNaiveInit[f\"Number of partition\" + str(nSlice)] = logNaiveInit\n",
    "    totalLogNaiveKmeans[f\"Number of partition\" + str(nSlice)] = logNaiveKmeans\n",
    "    tDurationsNaive[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsNaive[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "    for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "    print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "#### TOTAL OUTPUT ON FILE ####\n",
    "\n",
    "# compute the total log\n",
    "logNaive = {\"totalLogNaiveInit\": totalLogNaiveInit, \"totalLogNaiveKmeans\": totalLogNaiveKmeans, \"tDurationsNaive\": tDurationsNaive, \"tPreOperationsNaive\": tPreOperationsNaive}\n",
    "\n",
    "# save the log file\n",
    "if not os.path.exists('dataR'): # create a directory if it doesnt exist\n",
    "    os.makedirs('dataR')\n",
    "\n",
    "with open(pickle_fileR, \"wb\") as filer:\n",
    "    pickle.dump(logNaive, filer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e6cb7aa-391b-4378-8e4f-855ce1e6a8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tDurationsParallel': {'Number of partition2': 1971.7541599273682,\n",
      "                        'Number of partition32': 496.0981845855713,\n",
      "                        'Number of partition8': 496.9310529232025},\n",
      " 'tPreOperationsParallel': {'Number of partition2': 2.531020164489746,\n",
      "                            'Number of partition32': 5.254508018493652,\n",
      "                            'Number of partition8': 3.4235100746154785},\n",
      " 'totalLogParallelInit': {'Number of partition2': {'CostInit': [288492.0318876942,\n",
      "                                                                6471.204760221788,\n",
      "                                                                1659.288547267412,\n",
      "                                                                1006.116186481773,\n",
      "                                                                753.4967844848513,\n",
      "                                                                567.9758850353471,\n",
      "                                                                469.1507210647777,\n",
      "                                                                391.89928191987315,\n",
      "                                                                331.3038053637682,\n",
      "                                                                292.8420811737779,\n",
      "                                                                258.18729653330536,\n",
      "                                                                230.29641394019794,\n",
      "                                                                209.8539256768633],\n",
      "                                                   'tCentroids': [4.519091606140137,\n",
      "                                                                  13.845255136489868,\n",
      "                                                                  30.150264739990234,\n",
      "                                                                  47.893046855926514,\n",
      "                                                                  76.94650673866272,\n",
      "                                                                  110.6277072429657,\n",
      "                                                                  144.8072738647461,\n",
      "                                                                  194.2048144340515,\n",
      "                                                                  241.7678918838501,\n",
      "                                                                  291.29039478302,\n",
      "                                                                  351.40448021888733,\n",
      "                                                                  412.305668592453],\n",
      "                                                   'tSamples': [0.2103745937347412,\n",
      "                                                                0.19807767868041992,\n",
      "                                                                0.2174685001373291,\n",
      "                                                                0.20639562606811523,\n",
      "                                                                0.20718908309936523,\n",
      "                                                                0.20169448852539062,\n",
      "                                                                0.22543621063232422,\n",
      "                                                                0.20005440711975098,\n",
      "                                                                0.20611834526062012,\n",
      "                                                                0.20556092262268066,\n",
      "                                                                0.20529723167419434,\n",
      "                                                                0.20795893669128418],\n",
      "                                                   'tTotal': 1961.9151864051819},\n",
      "                          'Number of partition32': {'CostInit': [119127.78239709864,\n",
      "                                                                 3313.5227428921235,\n",
      "                                                                 1189.9708253271294,\n",
      "                                                                 714.4977523677312,\n",
      "                                                                 579.1044348104037,\n",
      "                                                                 471.25385565201304,\n",
      "                                                                 373.7063949555953,\n",
      "                                                                 310.4783001180102,\n",
      "                                                                 270.3815441908725,\n",
      "                                                                 240.65906177543437,\n",
      "                                                                 216.57160700260508,\n",
      "                                                                 193.91169868422304],\n",
      "                                                    'tCentroids': [2.2717199325561523,\n",
      "                                                                   5.618410587310791,\n",
      "                                                                   10.399859428405762,\n",
      "                                                                   16.60551929473877,\n",
      "                                                                   24.97147250175476,\n",
      "                                                                   33.146620750427246,\n",
      "                                                                   46.14911222457886,\n",
      "                                                                   59.13678193092346,\n",
      "                                                                   71.44770121574402,\n",
      "                                                                   86.24409604072571,\n",
      "                                                                   107.98920059204102],\n",
      "                                                    'tSamples': [0.42160701751708984,\n",
      "                                                                 0.4815826416015625,\n",
      "                                                                 0.5010690689086914,\n",
      "                                                                 0.49297642707824707,\n",
      "                                                                 0.5302729606628418,\n",
      "                                                                 0.4974055290222168,\n",
      "                                                                 0.49641871452331543,\n",
      "                                                                 0.5593862533569336,\n",
      "                                                                 0.49954748153686523,\n",
      "                                                                 0.42353320121765137,\n",
      "                                                                 0.4587883949279785],\n",
      "                                                    'tTotal': 484.33803939819336},\n",
      "                          'Number of partition8': {'CostInit': [119127.7814928484,\n",
      "                                                                4736.963323959286,\n",
      "                                                                1302.8713964349433,\n",
      "                                                                824.2747959054386,\n",
      "                                                                630.4697748589889,\n",
      "                                                                465.4578788758631,\n",
      "                                                                358.762929441606,\n",
      "                                                                298.8628194248406,\n",
      "                                                                258.7655819194407,\n",
      "                                                                235.4417765772506,\n",
      "                                                                209.85634176845807,\n",
      "                                                                194.15469774959692],\n",
      "                                                   'tCentroids': [1.9885032176971436,\n",
      "                                                                  5.656754970550537,\n",
      "                                                                  9.75054669380188,\n",
      "                                                                  16.45833444595337,\n",
      "                                                                  25.068891048431396,\n",
      "                                                                  37.37074685096741,\n",
      "                                                                  47.03401279449463,\n",
      "                                                                  60.680474042892456,\n",
      "                                                                  74.98714780807495,\n",
      "                                                                  90.4616596698761,\n",
      "                                                                  103.57710981369019],\n",
      "                                                   'tSamples': [0.26573610305786133,\n",
      "                                                                0.27621984481811523,\n",
      "                                                                0.27324748039245605,\n",
      "                                                                0.23257827758789062,\n",
      "                                                                0.30634117126464844,\n",
      "                                                                0.24586009979248047,\n",
      "                                                                0.23851537704467773,\n",
      "                                                                0.270322322845459,\n",
      "                                                                0.25359487533569336,\n",
      "                                                                0.2829160690307617,\n",
      "                                                                0.22051048278808594],\n",
      "                                                   'tTotal': 489.68595123291016}},\n",
      " 'totalLogParallelKmeans': {'Number of partition2': {'CostsKmeans': [2622.2390983349396,\n",
      "                                                                     2608.7449945762746],\n",
      "                                                     'tIterations': [4.123185157775879,\n",
      "                                                                     3.1782546043395996],\n",
      "                                                     'tTotal': 7.302965879440308},\n",
      "                            'Number of partition32': {'CostsKmeans': [2124.350525229758,\n",
      "                                                                      2116.3499635840494],\n",
      "                                                      'tIterations': [3.700284242630005,\n",
      "                                                                      2.795524835586548],\n",
      "                                                      'tTotal': 6.49947452545166},\n",
      "                            'Number of partition8': {'CostsKmeans': [2626.601985687509,\n",
      "                                                                     2614.206495919551],\n",
      "                                                     'tIterations': [1.9223363399505615,\n",
      "                                                                     1.8908495903015137],\n",
      "                                                     'tTotal': 3.815715789794922}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "with open(pickle_fileP, \"rb\") as f:\n",
    "    logParallel = pickle.load(f)\n",
    "\n",
    "pprint.pprint(logParallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe256ad-50fa-4ebe-9f8f-7d2b475289b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tDurationsNaive': {'Number of partition2': 16.084462881088257,\n",
      "                     'Number of partition32': 19.236206531524658,\n",
      "                     'Number of partition8': 15.17403531074524},\n",
      " 'tPreOperationsNaive': {'Number of partition2': 2.501878023147583,\n",
      "                         'Number of partition32': 3.021709442138672,\n",
      "                         'Number of partition8': 1.8723645210266113},\n",
      " 'totalLogNaiveInit': {'Number of partition2': {'tTotal': 0.668297290802002},\n",
      "                       'Number of partition32': {'tTotal': 1.3324108123779297},\n",
      "                       'Number of partition8': {'tTotal': 0.5763225555419922}},\n",
      " 'totalLogNaiveKmeans': {'Number of partition2': {'CostsKmeans': [7985.981401562164,\n",
      "                                                                  5077.997175826575,\n",
      "                                                                  4436.048363155805,\n",
      "                                                                  4308.379820967888,\n",
      "                                                                  4306.069707688095],\n",
      "                                                  'tIterations': [3.165806770324707,\n",
      "                                                                  2.560190439224243,\n",
      "                                                                  2.342775583267212,\n",
      "                                                                  2.4832041263580322,\n",
      "                                                                  2.3573668003082275],\n",
      "                                                  'tTotal': 12.914111852645874},\n",
      "                         'Number of partition32': {'CostsKmeans': [7015.593381431981,\n",
      "                                                                   4689.023066173977,\n",
      "                                                                   4230.277020679714,\n",
      "                                                                   3861.1054274172507,\n",
      "                                                                   3699.000455304387,\n",
      "                                                                   3689.5813291097966],\n",
      "                                                   'tIterations': [2.72788143157959,\n",
      "                                                                   2.5331737995147705,\n",
      "                                                                   2.433570623397827,\n",
      "                                                                   2.4328525066375732,\n",
      "                                                                   2.468374490737915,\n",
      "                                                                   2.2787747383117676],\n",
      "                                                   'tTotal': 14.88192367553711},\n",
      "                         'Number of partition8': {'CostsKmeans': [17329.460543691042,\n",
      "                                                                  7562.3869822212655,\n",
      "                                                                  5469.286163360544,\n",
      "                                                                  5036.036618591531,\n",
      "                                                                  4774.126761002581,\n",
      "                                                                  4549.93829228687,\n",
      "                                                                  4470.413189154786,\n",
      "                                                                  4464.835982184708],\n",
      "                                                  'tIterations': [1.668227195739746,\n",
      "                                                                  1.6367926597595215,\n",
      "                                                                  1.5867080688476562,\n",
      "                                                                  1.500199556350708,\n",
      "                                                                  1.6272165775299072,\n",
      "                                                                  1.5547795295715332,\n",
      "                                                                  1.5702154636383057,\n",
      "                                                                  1.5691428184509277],\n",
      "                                                  'tTotal': 12.725183963775635}}}\n"
     ]
    }
   ],
   "source": [
    "with open(pickle_fileR, \"rb\") as f:\n",
    "    logParallel = pickle.load(f)\n",
    "\n",
    "pprint.pprint(logParallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be8588-13da-4d15-b551-3b64462412b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill spark and the context\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
