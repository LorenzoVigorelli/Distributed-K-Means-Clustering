{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b7eed2-1d38-40a6-825a-d9fc201f29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from pyspark.sql import SparkSession\n",
    "from time import time, sleep\n",
    "import subprocess\n",
    "import pprint\n",
    "import sys\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2500614-e910-4c13-b4bd-033ecca6017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the spark warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "logging.getLogger('py4j').setLevel(logging.ERROR) \n",
    "logging.getLogger('pyspark').setLevel(logging.ERROR) \n",
    "log4j_conf_path = \"file:///home/quivigorelli/Distributed-K-Means-Clustering/spark/DistributedKmeans/log4j.properties\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fb5bf-cf09-44b6-81ec-4cedb6529c70",
   "metadata": {},
   "source": [
    "Global hyperparameters and data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826cb1c4-f323-43b0-a155-f6dec45e08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files for reading and writing of data dictionaries\n",
    "pickle_fileP = 'dataP/log1P_C10.pkl' # Parallel initialization data\n",
    "pickle_fileR = 'dataR/log1U_C10.pkl' # Random initialization data\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "np.random.seed(12345)\n",
    "spark_seed = 54321\n",
    "\n",
    "# Number of partitions \n",
    "nSlices = [128] # done 2, 4, 8, 16, 32, 64, 128\n",
    "\n",
    "# Size of considered subset\n",
    "subLen = 300_000\n",
    "\n",
    "# Maximum number of iterations in Lloyds algorithm\n",
    "lloydsMaxIterations=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1426166-735a-4146-9df4-3bf35d3e6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def labelToInt(label):\n",
    "    '''\n",
    "    Map from set of labels in original dataset (`strings`) into set of natural numbers (`int`) for easier manipulation of rdd\n",
    "    '''\n",
    "    uniqueLabels=list(np.unique(y))\n",
    "    return uniqueLabels.index(label)\n",
    "\n",
    "\n",
    "def deleteBytes(datum):\n",
    "    '''\n",
    "    Clean dataset from categorical attributes, leaving numerical ones\n",
    "    Arguments:\n",
    "    One datum of the rdd.\n",
    "    Return:\n",
    "    Updated datum.\n",
    "    '''\n",
    "    x = datum[1][\"x\"]\n",
    "    mask = [type(i) != bytes for i in x]\n",
    "    datum[1][\"x\"] = np.asarray(x[mask])\n",
    "    print(x)\n",
    "    print(mask)\n",
    "    return datum\n",
    "\n",
    "\n",
    "def localPlusPlusInit(points, k): \n",
    "    '''\n",
    "    KMeans++ initialization.\n",
    "    Arguments:\n",
    "    `points`: array (n, dim) of points to be clustered;\n",
    "    `k`: desired number of centroids. \n",
    "    Returns:\n",
    "    Initial array (k, dim) of centroids, k<=n.\n",
    "    '''\n",
    "    # Sample one point uniformly from points array\n",
    "    C=points[np.random.choice(points.shape[0])]\n",
    "    C=C[np.newaxis, :]\n",
    "    \n",
    "    for _ in range(k):\n",
    "        # Compute array (n,1) of probabilities associated to each point\n",
    "        probs=np.min(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1).flatten()\n",
    "        # Normalize probability distribution\n",
    "        probs=probs/np.sum(probs)\n",
    "        \n",
    "        # Draw one new centroid according to distrbution\n",
    "        nextCentroid=points[np.random.choice(points.shape[0], p=probs)][np.newaxis,:]\n",
    "        # Add centroid to array\n",
    "        C=np.vstack((C, nextCentroid))\n",
    "    return C\n",
    "\n",
    "\n",
    "def weightedAverage(group):\n",
    "    \"\"\"\n",
    "    Compute weighted average of a group from a pd.DataFrame with point coordinates, weights, clusterId.\n",
    "    Utilized in local (non-distributed) version of Lloyds algorithm, needed also in K-Means//\n",
    "    \"\"\"\n",
    "    weight_column='weights'\n",
    "    groupby_column='clusterId'\n",
    "    columns_to_average = group.columns.difference([weight_column, groupby_column])\n",
    "    weighted_averages = group[columns_to_average].multiply(group[weight_column], axis=0).sum() / group[weight_column].sum()\n",
    "    return weighted_averages\n",
    "\n",
    "\n",
    "def localLloyds(points, k, C_init=None, weights=None, n_iterations=100, logDict=None):\n",
    "    \"\"\"\n",
    "    Local (non-distributed) Lloyds algorithm\n",
    "    Arguments:\n",
    "    `points`: array (n, dim) of points to cluster;\n",
    "    `k`: number of desired clusters;\n",
    "    `C_init`: optional, array (k, dim) of initial centroids\n",
    "    `weights`: optional, weights for weighted average in centroid re-computing;\n",
    "    `n_iterations`: optional, number of iteration in lloyds algorithm;\n",
    "    `logDict`: optional, dictionary {'CostsKmeans', 'tIterations', 'tTotal'} to store cost and time info.\n",
    "    Return:\n",
    "    Array of expected centroids.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "\n",
    "    # Storing cost and time info\n",
    "    my_kMeansCosts = []\n",
    "    tIterations = []\n",
    "    \n",
    "    df=pd.DataFrame(points)\n",
    "\n",
    "    # If weights not given, assume uniform weights for points\n",
    "    if weights is None:\n",
    "        weights=np.ones(shape=len(points))\n",
    "    df['weights']=weights\n",
    "    df['clusterId']=np.zeros(shape=len(points))\n",
    "\n",
    "    # If no C_init, default to K-Means++ initialization\n",
    "    if C_init is None:\n",
    "        C=localPlusPlusInit(points, k)\n",
    "    else:\n",
    "        C=C_init\n",
    "   \n",
    "    clusterId=np.argmin(np.sum((points[:,:,np.newaxis]-C.T[np.newaxis,:,:])**2, axis=1), axis=1)\n",
    "    for iteration in range(n_iterations):\n",
    "        t1=time()\n",
    "\n",
    "        # Compute centroid given cluster\n",
    "        df['clusterId']=clusterId\n",
    "        C_df=df.groupby('clusterId')\\\n",
    "            .apply(weightedAverage)\\\n",
    "            .reset_index()\n",
    "\n",
    "        # Compute cluster given centroid\n",
    "        C_array=C_df[C_df.columns.difference(['weights', 'clusterId'])].reset_index(drop=True).to_numpy()\n",
    "        squared_distances=np.sum((points[:,:,np.newaxis]-C_array.T[np.newaxis,:,:])**2, axis=1)\n",
    "        clusterId=np.argmin(squared_distances, axis=1)\n",
    "        my_cost=sum(squared_distances[np.arange(len(squared_distances)), clusterId])\n",
    "\n",
    "        my_kMeansCosts.append(my_cost)\n",
    "        t2 = time()\n",
    "        \n",
    "        tIteration = t2 - t1\n",
    "        tIterations.append(tIteration)\n",
    "\n",
    "    tEnd = time()\n",
    "    tTotal = tEnd - t0\n",
    "\n",
    "    # Store cost and time info\n",
    "    if logDict is not None:\n",
    "        logDict[\"CostsKmeans\"] = my_kMeansCosts\n",
    "        logDict[\"tIterations\"] = tIterations\n",
    "        logDict[\"tTotal\"] = tTotal\n",
    "    \n",
    "    return C_array \n",
    "\n",
    "\n",
    "def minmaxRescale(datum, minS, maxS):\n",
    "    \"\"\"\n",
    "    Rescale datum in [0,1] interval for better clusterization\n",
    "    Arguments:\n",
    "    `datum`: see rdd format;\n",
    "    `minS`: array of min coordinate value among points for each attribute;\n",
    "    `maxS`: as `minS` with max.\n",
    "    Return:\n",
    "    Updated datum.\n",
    "    \"\"\"\n",
    "    mask = np.array(minS < maxS).astype(bool)\n",
    "    feature = datum[1][\"x\"] \n",
    "    feature = (feature[mask] - minS[mask])/(maxS[mask] - minS[mask])\n",
    "    return (datum[0], {\"x\": feature, \"y\": datum[1][\"y\"], \"d2\":datum[1][\"d2\"]}) \n",
    "\n",
    "\n",
    "def selectCluster(datum, C, updateDistances=True):\n",
    "    \"\"\"\n",
    "    Associate datum to its centroid and optionally updates squared distance between them.\n",
    "    Arguments:\n",
    "    `datum`: see rdd format;\n",
    "    `C`: array (k, len(datum[1][\"x\"]));\n",
    "    `updateDistances`: if True, updates `datum[1][\"d2\"]` with squared distance between datum point and closest centroid in C.\n",
    "    Return:\n",
    "    Updated datum.\n",
    "    \"\"\"\n",
    "    distances = np.sum((datum[1][\"x\"] - C)**2, axis=1)\n",
    "    print('distances: ',distances)\n",
    "    clusterId = np.argmin(distances)\n",
    "    if updateDistances is True:\n",
    "        return (clusterId, {'x':datum[1]['x'], 'y':datum[1]['y'], 'd2':distances[clusterId]})\n",
    "    else:\n",
    "        return (clusterId, datum[1])\n",
    "\n",
    "\n",
    "def updateCentroids(Rdd):\n",
    "    \"\"\"\n",
    "    Update centroids as spatial average of cluster points\n",
    "    Argument:\n",
    "    `Rdd`: see rdd format;\n",
    "    Return:\n",
    "    Updated array of centroids.\n",
    "    \"\"\"\n",
    "    C=Rdd.mapValues(lambda xy: (xy['x'], 1))\\\n",
    "              .reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1]))\\\n",
    "              .mapValues(lambda a:a[0]/a[1])\\\n",
    "              .values()\\\n",
    "              .collect() \n",
    "    C=np.array(C) #check later more carefully if causes some overhead\n",
    "    return C\n",
    "\n",
    "\n",
    "def updateDistances(Rdd, C):\n",
    "    \"\"\"\n",
    "    Update Rdd with square distances from centroids, given Rdd with clusters already assigned to each point\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd format;\n",
    "    `C`: array of cluster centroids.\n",
    "    Return:\n",
    "    Updated rdd.\n",
    "    \"\"\"\n",
    "    def datumUpdate(datum, C):\n",
    "        '''\n",
    "        Update a datum of the rdd with distance from assigned centroid\n",
    "        '''\n",
    "        d2=np.sum((datum[1]['x']-C[datum[0]])**2)\n",
    "        #return datum\n",
    "        return (datum[0], {\"x\": datum[1][\"x\"], \"y\": datum[1][\"y\"], \"d2\":d2})\n",
    "    Rdd=Rdd.map(lambda datum:datumUpdate(datum, C))\n",
    "    return Rdd\n",
    "\n",
    "\n",
    "def cost(Rdd):\n",
    "    \"\"\"\n",
    "    Calculate global cost of clusterization, from an Rdd with distances from centroids already updated\n",
    "    \"\"\"\n",
    "    my_cost=Rdd.map(lambda datum : datum[1]['d2'])\\\n",
    "               .reduce(lambda a,b: a+b)\n",
    "    return my_cost \n",
    "\n",
    "\n",
    "def kMeans(Rdd, C_init, maxIterations, logParallelKmeans=None):\n",
    "    \"\"\"\n",
    "    Distributed (parallel) Lloyds algorithm\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd format;\n",
    "    `C_init`: array (k, dim) of initial centroids;\n",
    "    `maxIterations`: max number of iterations;\n",
    "    `logParallelKmeans`: optional, dictionary {'CostsKmeans', 'tIterations', 'tTotal'} to store cost and time info.\n",
    "    Return:\n",
    "    Array of expected centroids.\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time()\n",
    "\n",
    "    # Storing cost and time info\n",
    "    my_kMeansCosts = []\n",
    "    tIterations = []\n",
    "    C=C_init\n",
    "\n",
    "    for t in range(maxIterations):\n",
    "        t1 = time()\n",
    "        RddCached = Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "        \n",
    "        # Now we compute the new centroids by calculating the averages of points belonging to the same cluster.\n",
    "        C=updateCentroids(RddCached)\n",
    "        my_cost = cost(RddCached)\n",
    "        \n",
    "        my_kMeansCosts.append(my_cost)\n",
    "        t2 = time()\n",
    "        \n",
    "        tIteration = t2 - t1\n",
    "        tIterations.append(tIteration)\n",
    "        \n",
    "        #RddCached.unpersist() \n",
    "\n",
    "        # Break loop if convergence of cost is reached\n",
    "        if (len(my_kMeansCosts) > 1) and (my_kMeansCosts[-1] > 0.999*my_kMeansCosts[-2]):\n",
    "            break\n",
    "\n",
    "    tEnd = time()\n",
    "    tTotal = tEnd - t0\n",
    "\n",
    "    # Store cost and time info in argument dictionary\n",
    "    if logParallelKmeans is not None:\n",
    "        logParallelKmeans[\"CostsKmeans\"] = my_kMeansCosts\n",
    "        logParallelKmeans[\"tIterations\"] = tIterations\n",
    "        logParallelKmeans[\"tTotal\"] = tTotal\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def naiveInitFromSet(Rdd, k, logNaiveInit=None, spark_seed=54321):\n",
    "    \"\"\"\n",
    "    Uniform sampling of k points from Rdd\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd structure;\n",
    "    `k`: desired number of clusters;\n",
    "    `spark_seed`: optional, seed for spark random sampling;\n",
    "    `logNaiveInit`: optional, dictionary {'tTotal'} to store time info.\n",
    "    Return:\n",
    "    Initial array (k, dim) of centroids.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    # Sampling. Replacement is set to False to avoid coinciding centroids BUT no guarantees that in the original dataset all points are distinct!!!\n",
    "    kSubset=Rdd.takeSample(False, k, seed=spark_seed)\n",
    "    C_init=np.array([datum[1]['x'] for datum in kSubset])\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logNaiveInit is not None:\n",
    "        logNaiveInit[\"tTotal\"] = tEnd - t0\n",
    "        \n",
    "    return C_init\n",
    "\n",
    "\n",
    "def naiveInitFromSpace(k, dim):\n",
    "    \"\"\"\n",
    "    Uniform drawing of k points from euclidean space assuming the Rdd has been mapped into a [0,1]^dim space\n",
    "    Arguments:\n",
    "    `k`: desired number of clusters;\n",
    "    `dim`: dimensionality of points space.\n",
    "    Return:\n",
    "    Initial array (k, dim) of centroids.\n",
    "    \"\"\"\n",
    "    C_init=np.random.uniform(size=(k,dim))\n",
    "    return C_init\n",
    "\n",
    "\n",
    "def parallelInit(Rdd, k, l, logParallelInit=None):\n",
    "    \"\"\"\n",
    "    Parallel initialization\n",
    "    Arguments:\n",
    "    `Rdd`: see rdd structure;\n",
    "    `k`: desired number of clusters;\n",
    "    `l`: coefficient to adjust sampling probability in order to obtain at least k centroids;\n",
    "    `logParallelInit`: optional, dictionary {'CostsKmeans', 'tIterations', 'tTotal'} to store cost and time info.\n",
    "    Return:\n",
    "    Initial array (k, dim) of centroids.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    # initialize C as a point in the dataset\n",
    "    C=naiveInitFromSet(Rdd, 1) \n",
    "    \n",
    "    # associate each datum to the only centroid (computed before) and computed distances and cost\n",
    "    Rdd=Rdd.map(lambda datum : (0, datum[1]))\n",
    "    Rdd=updateDistances(Rdd, C).persist() ###\n",
    "    \n",
    "    my_cost=cost(Rdd)\n",
    "\n",
    "    # number of iterations (log(cost))\n",
    "    n_iterations=int(np.log(my_cost))\n",
    "    if(n_iterations<1): n_iterations=1\n",
    "    \n",
    "    tSamples = []\n",
    "    tCentroids = []\n",
    "    CostInits = [my_cost]\n",
    "    # iterative sampling of the centroids\n",
    "    for _ in range(n_iterations):\n",
    "\n",
    "        t1=time()\n",
    "        # sample C' according to the probability\n",
    "        C_prime=Rdd.filter(lambda datum : np.random.uniform()<l*datum[1]['d2']/my_cost)\\\n",
    "                   .map(lambda datum : datum[1]['x'])\\\n",
    "                   .collect()\n",
    "        C_prime=np.array(C_prime)\n",
    "        t2=time()\n",
    "\n",
    "        # stack C and C', update distances, centroids, and cost\n",
    "        if (C_prime.shape[0]>0):\n",
    "            C=np.vstack((C, C_prime))\n",
    "            \n",
    "            #Rdd.unpersist() ###\n",
    "            Rdd=Rdd.map(lambda datum: selectCluster(datum, C)).persist() ###\n",
    "            \n",
    "            my_cost=cost(Rdd)\n",
    "        t3=time()\n",
    "\n",
    "        tSample = t2 -t1\n",
    "        tCentroid = t3 - t2\n",
    "        tSamples.append(tSample)\n",
    "        tCentroids.append(tCentroid)\n",
    "        CostInits.append(my_cost)\n",
    "       \n",
    "    #erase centroids sampled more than once \n",
    "    C=C.astype(float)\n",
    "    C=np.unique(C, axis=0)\n",
    "    Rdd=Rdd.map(lambda datum: selectCluster(datum, C))\n",
    "    \n",
    "    #compute weights of centroids (sizes of each cluster) and put them in a list whose index is same centroid index as C\n",
    "    wx=Rdd.countByKey()\n",
    "    weights=np.zeros(len(C))\n",
    "    weights[[list(wx.keys())]]=[list(wx.values())]\n",
    "    \n",
    "    #subselection of k centroids from C, using local Lloyds algorithm with k-means++ initialization\n",
    "    if C.shape[0]<=k:\n",
    "        C_init=C\n",
    "    else:\n",
    "        C_init=localLloyds(C, k, weights=weights, n_iterations=100) #can be set to lloydsMaxIterations for consistency TODO\n",
    "\n",
    "    tEnd = time()\n",
    "    \n",
    "    if logParallelInit is not None:\n",
    "        logParallelInit[\"tSamples\"] = tSamples\n",
    "        logParallelInit[\"tCentroids\"] = tCentroids\n",
    "        logParallelInit[\"CostInit\"] = CostInits\n",
    "        logParallelInit[\"tTotal\"] = tEnd - t0\n",
    "\n",
    "    #Rdd.unpersist() ###\n",
    "    return C_init\n",
    "\n",
    "def predictedCentroidsLabeler(C_expected, C_predicted):\n",
    "    \"\"\"\n",
    "    Associate expected and predicted centroids based on distance.\n",
    "    Parameters:\n",
    "    `C_expected`: array (k, dim) of expected centroids;\n",
    "    `C_predicted`: array (k,dim) of predicted centroids;\n",
    "    Return:\n",
    "    List of labels, one for each expected centroid and pointing to its nearest predicted centroid;\n",
    "    List of corresponding distances.\n",
    "    \"\"\"\n",
    "    # Compute the distance matrix\n",
    "    distMatrix=np.sum((C_expected[:,:,np.newaxis]-C_predicted.T[np.newaxis, :,:])**2,axis=1)\n",
    "    # The labeler i-th entry j, tells that i-th centroid of C_expected is associated to j-th element of C_predicted\n",
    "    labeler=np.argmin(distMatrix,axis=1)\n",
    "    # Square distance of element of C_expected to nearest point in C_predicted\n",
    "    distances=np.sqrt(np.array(distMatrix[np.arange(len(distMatrix)),labeler]).astype(float))\n",
    "    return labeler, distances\n",
    "\n",
    "\n",
    "def nearestCentroidDistances(C):\n",
    "    \"\"\"\n",
    "    Associate each centroid to the distance of the nearest one\n",
    "    Parameters:\n",
    "    `C`:  array (k, dim) of centroids;\n",
    "    Return:\n",
    "    List of labels, one for each centroid and pointing to its nearest centroid;\n",
    "    List of corresponding distances.\n",
    "    \"\"\"\n",
    "    # Compute the distance matrix\n",
    "    distMatrix=np.sum((C[:,:,np.newaxis]-C.T[np.newaxis, :,:])**2,axis=1)\n",
    "    distMatrix+=np.diag(np.repeat(np.inf, distMatrix.shape[0]))\n",
    "    \n",
    "    # The labeler i-th entry j, tells that i-th centroid of C_expected is associated to j-th element of C_predicted\n",
    "    labeler=np.argmin(distMatrix,axis=1)\n",
    "    \n",
    "    # Square distance of element of C_expected to nearest point in C_predicted\n",
    "    distances=np.sqrt(np.array(distMatrix[np.arange(distMatrix.shape[0]),labeler]).astype(float))\n",
    "    return labeler, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741807da-d71c-4192-973b-3207e1d23b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/08 17:10:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 128 number of partition started at time 1720458659.9260516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 19.820308446884155 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 978.1839804649353 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 128 number of partition ended at time 1720459786.541283 after 1126.6152312755585 seconds\n",
      "The iteration with 128 number of partition started at time 1720459788.0334823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the pre-steps after 14.513504028320312 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the initialization after 5.651534557342529 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteration with 128 number of partition ended at time 1720460034.743066 after 246.70958375930786 seconds\n",
      "CPU times: user 9.29 s, sys: 1.37 s, total: 10.7 s\n",
      "Wall time: 22min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### SPARK SETUP ###\n",
    "\n",
    "# Build a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"Clustering\")\\\n",
    "    .config(\"spark.executor.memory\", \"7g\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Dlog4j.configuration=file:{log4j_conf_path}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Eventually clear old data (if re-running)\n",
    "spark.catalog.clearCache() \n",
    "# for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "#     rdd.unpersist()\n",
    "\n",
    "#### IMPORT THE DATA SET ####\n",
    "data = fetch_kddcup99(return_X_y = True, percent10 = True) # default percent10=True\n",
    "\n",
    "# collect samples and features (target)\n",
    "x = data[0]\n",
    "y = data[1] \n",
    "\n",
    "# Shuffle\n",
    "shuffled_indices = np.random.permutation(len(x))\n",
    "x=x[shuffled_indices]\n",
    "y=y[shuffled_indices]\n",
    "\n",
    "# cut the data fro memory reasons\n",
    "x = x[:subLen,]\n",
    "y = y[:subLen]\n",
    "\n",
    "for nSlice in nSlices:\n",
    "    ### PARALLEL ###\n",
    "\n",
    "    # Open file if exists\n",
    "    sleep(1)\n",
    "    if os.path.isfile(pickle_fileP):\n",
    "        with open(pickle_fileP, \"rb\") as f:\n",
    "            logParallel = pickle.load(f)\n",
    "            totalLogParallelInit, totalLogParallelKmeans, tDurationsParallel, tPreOperationsParallel = logParallel.values()\n",
    "    else:\n",
    "        totalLogParallelInit = {}\n",
    "        totalLogParallelKmeans = {}\n",
    "        tDurationsParallel = {}\n",
    "        tPreOperationsParallel = {}\n",
    "\n",
    "    # Start the algorithm\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # Parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # Cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # Setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # Rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # Setting up the input and output information for the algorithm\n",
    "    logParallelInit = {}\n",
    "    logParallelKmeans = {}\n",
    "\n",
    "    # Setup k and l\n",
    "    k=kTrue\n",
    "    l=k*2 \n",
    "    \n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # Initialization kMeans //\n",
    "    C_init = parallelInit(Rdd, k, l, logParallelInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # Run the k-means alghoritm\n",
    "    C = kMeans(Rdd, C_init, lloydsMaxIterations, logParallelKmeans)\n",
    "    \n",
    "    # Time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # Output in the correct memory adresses\n",
    "    totalLogParallelInit[f\"Number of partition\" + str(nSlice)] = logParallelInit\n",
    "    totalLogParallelKmeans[f\"Number of partition\" + str(nSlice)] = logParallelKmeans\n",
    "    tDurationsParallel[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsParallel[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    #Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    # for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    #     rdd.unpersist()\n",
    "    # print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "\n",
    "    # Compute the total log\n",
    "    logParallel = {\"totalLogParallelInit\": totalLogParallelInit, \"totalLogParallelKmeans\": totalLogParallelKmeans, \"tDurationsParallel\": tDurationsParallel, \"tPreOperationsParallel\": tPreOperationsParallel}\n",
    "    \n",
    "    # Save the log file\n",
    "    if not os.path.exists('dataP'): # create a directory if it doesnt exist\n",
    "        os.makedirs('dataP')\n",
    "    \n",
    "    with open(pickle_fileP, \"wb\") as file:\n",
    "        pickle.dump(logParallel, file)\n",
    "\n",
    "    # Clear the space\n",
    "    subprocess.run(\"ssh slave2 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "    subprocess.run(\"ssh slave3 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "\n",
    "\n",
    "    ### NAIVE INIT ###\n",
    "    \n",
    "    # Load log if it exists\n",
    "    sleep(1)\n",
    "    if os.path.isfile(pickle_fileR):\n",
    "        with open(pickle_fileR, \"rb\") as f:\n",
    "            logNaive = pickle.load(f)\n",
    "            totalLogNaiveInit, totalLogNaiveKmeans, tDurationsNaive, tPreOperationsNaive = logNaive.values()\n",
    "    else:\n",
    "        totalLogNaiveInit = {}\n",
    "        totalLogNaiveKmeans = {}\n",
    "        tDurationsNaive = {}\n",
    "        tPreOperationsNaive = {}\n",
    "    \n",
    "    # Start algo\n",
    "    tInit = time() # compute the time of the beginning of the iteration over the number of partitions\n",
    "    print(f\"The iteration with {nSlice} number of partition started at time {tInit}\")\n",
    "    \n",
    "    # Parallelize over nSlice partitions\n",
    "    Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)\n",
    "\n",
    "    # Cut the categorical attributes\n",
    "    Rdd = Rdd.map(deleteBytes)\\\n",
    "             .persist()\n",
    "\n",
    "    # Setting the theoretical number of clusters\n",
    "    kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "               .distinct()\\\n",
    "               .count()\n",
    "    \n",
    "    # Rescale the RDD over the max\n",
    "    maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "    minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "    Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "             .persist()\n",
    "    \n",
    "    # Setting up the input and output information for the algorithm\n",
    "    logNaiveInit = {}\n",
    "    logNaiveKmeans = {}\n",
    "\n",
    "    # Setup k and l\n",
    "    k=kTrue\n",
    "    l=k*2 \n",
    "    \n",
    "    tInitI = time()\n",
    "\n",
    "    tPreOperation = tInitI - tInit\n",
    "    print(f\"Finished the pre-steps after {tPreOperation} seconds\")\n",
    "          \n",
    "    # initialization kMeans//\n",
    "    C_init = naiveInitFromSet(Rdd, k, logNaiveInit)\n",
    "    \n",
    "    tInitialization = time() - tInitI\n",
    "    print(f\"Finished the initialization after {tInitialization} seconds\")\n",
    "    \n",
    "    # Run the k-means algorithm\n",
    "    C = kMeans(Rdd, C_init, lloydsMaxIterations, logNaiveKmeans)\n",
    "    \n",
    "    # Time information\n",
    "    tEnd = time() # compute the time of the end of the iteration over the number of partitions\n",
    "    tDuration = tEnd - tInit\n",
    "    \n",
    "    print(f\"The iteration with {nSlice} number of partition ended at time {tEnd} after {tDuration} seconds\")\n",
    "\n",
    "    # Output in the correct memory adresses\n",
    "    totalLogNaiveInit[f\"Number of partition\" + str(nSlice)] = logNaiveInit\n",
    "    totalLogNaiveKmeans[f\"Number of partition\" + str(nSlice)] = logNaiveKmeans\n",
    "    tDurationsNaive[f\"Number of partition\" + str(nSlice)] = tDuration\n",
    "    tPreOperationsNaive[f\"Number of partition\" + str(nSlice)] = tPreOperation\n",
    "\n",
    "    #Rdd.unpersist()\n",
    "\n",
    "    spark.catalog.clearCache() \n",
    "    # for (id, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "    #     rdd.unpersist()\n",
    "    # print(\"Persisted RRDs: \", len(sc._jsc.getPersistentRDDs().items()))\n",
    "\n",
    "    # Compute the total log\n",
    "    logNaive = {\"totalLogNaiveInit\": totalLogNaiveInit, \"totalLogNaiveKmeans\": totalLogNaiveKmeans, \"tDurationsNaive\": tDurationsNaive, \"tPreOperationsNaive\": tPreOperationsNaive}\n",
    "    \n",
    "    # Save the log file\n",
    "    if not os.path.exists('dataR'): # create a directory if it doesnt exist\n",
    "        os.makedirs('dataR')\n",
    "    \n",
    "    with open(pickle_fileR, \"wb\") as filer:\n",
    "        pickle.dump(logNaive, filer)\n",
    "\n",
    "    # Clear the space\n",
    "    subprocess.run(\"ssh slave2 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)\n",
    "    subprocess.run(\"ssh slave3 'cd /usr/local/spark/work/ && [ \\\"$(ls -A .)\\\" ] && rm -r ./*'\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455f721f-cd5d-4f48-be41-66fddf85b878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'totalLogParallelInit': {'Number of partition8': {'tSamples': [0.5604074001312256,\n",
       "    0.7893469333648682,\n",
       "    1.0149109363555908,\n",
       "    0.7487082481384277,\n",
       "    0.7158007621765137,\n",
       "    0.7343785762786865,\n",
       "    0.6166317462921143,\n",
       "    0.8378362655639648,\n",
       "    0.6786437034606934,\n",
       "    0.6768665313720703,\n",
       "    0.6516311168670654,\n",
       "    0.7696502208709717,\n",
       "    0.8361482620239258],\n",
       "   'tCentroids': [11.381458282470703,\n",
       "    20.180540800094604,\n",
       "    26.88866639137268,\n",
       "    31.74018430709839,\n",
       "    38.81391096115112,\n",
       "    48.69937872886658,\n",
       "    53.620816230773926,\n",
       "    58.794254779815674,\n",
       "    67.91973447799683,\n",
       "    75.13777709007263,\n",
       "    82.12062764167786,\n",
       "    87.50198769569397,\n",
       "    91.46046209335327],\n",
       "   'CostInit': [891555.5554263061,\n",
       "    33927.19435053103,\n",
       "    10901.317924007788,\n",
       "    6135.548123954543,\n",
       "    4941.270404556077,\n",
       "    3649.814060380155,\n",
       "    2910.798577969736,\n",
       "    2545.3242604744655,\n",
       "    2226.1752166524716,\n",
       "    1975.2555548138662,\n",
       "    1796.6171079799242,\n",
       "    1681.1847633852371,\n",
       "    1534.9404615078295,\n",
       "    1445.2145687147777],\n",
       "   'tTotal': 812.2571785449982},\n",
       "  'Number of partition4': {'tSamples': [0.7486796379089355,\n",
       "    0.8389637470245361,\n",
       "    0.7579433917999268,\n",
       "    0.6595571041107178,\n",
       "    0.6617116928100586,\n",
       "    0.7932686805725098,\n",
       "    0.7865848541259766,\n",
       "    0.8459608554840088,\n",
       "    0.7660346031188965,\n",
       "    0.7702269554138184,\n",
       "    0.8567297458648682,\n",
       "    0.711101770401001,\n",
       "    0.7883894443511963,\n",
       "    0.809903621673584],\n",
       "   'tCentroids': [13.098650693893433,\n",
       "    27.931509256362915,\n",
       "    39.81945991516113,\n",
       "    53.59331917762756,\n",
       "    64.32080483436584,\n",
       "    93.69615769386292,\n",
       "    96.14066004753113,\n",
       "    108.745112657547,\n",
       "    122.67282629013062,\n",
       "    136.7574405670166,\n",
       "    150.79018878936768,\n",
       "    164.84979462623596,\n",
       "    179.89866662025452,\n",
       "    191.3278887271881],\n",
       "   'CostInit': [2223256.496517683,\n",
       "    58275.187236190715,\n",
       "    15117.338164315988,\n",
       "    7492.701184889861,\n",
       "    5491.0433568077115,\n",
       "    4141.556888892963,\n",
       "    3473.3583017584583,\n",
       "    2858.559858498532,\n",
       "    2487.5859838572815,\n",
       "    2162.1684975189987,\n",
       "    1919.9214738361075,\n",
       "    1765.4805436598567,\n",
       "    1616.20679054093,\n",
       "    1478.0450011882313,\n",
       "    1381.1972864673216],\n",
       "   'tTotal': 1666.448011636734},\n",
       "  'Number of partition2': {'tSamples': [1.120103359222412,\n",
       "    1.1706554889678955,\n",
       "    0.9483442306518555,\n",
       "    1.0482196807861328,\n",
       "    0.9928898811340332,\n",
       "    1.0721609592437744,\n",
       "    1.239293098449707,\n",
       "    1.1702768802642822,\n",
       "    1.203402042388916,\n",
       "    1.0181424617767334,\n",
       "    1.0759682655334473,\n",
       "    1.0064671039581299,\n",
       "    1.0744972229003906],\n",
       "   'tCentroids': [34.37669229507446,\n",
       "    56.87931299209595,\n",
       "    83.23446941375732,\n",
       "    107.59471559524536,\n",
       "    131.97507643699646,\n",
       "    157.73672437667847,\n",
       "    185.29766201972961,\n",
       "    206.84126925468445,\n",
       "    225.1283221244812,\n",
       "    249.35660481452942,\n",
       "    273.93789505958557,\n",
       "    305.92539501190186,\n",
       "    333.4616506099701],\n",
       "   'CostInit': [891555.5554243359,\n",
       "    34577.33079382057,\n",
       "    10674.196787091434,\n",
       "    6935.988986047896,\n",
       "    4799.194078541578,\n",
       "    3851.499789400862,\n",
       "    3203.844055363491,\n",
       "    2665.9284753856473,\n",
       "    2356.1538628193257,\n",
       "    2198.94533684475,\n",
       "    2002.930445091094,\n",
       "    1847.0223907129443,\n",
       "    1680.3068973200234,\n",
       "    1567.9206822287215],\n",
       "   'tTotal': 2744.77747297287},\n",
       "  'Number of partition16': {'tSamples': [0.8386671543121338,\n",
       "    0.8171069622039795,\n",
       "    0.8569900989532471,\n",
       "    0.7518148422241211,\n",
       "    0.8357596397399902,\n",
       "    0.7499816417694092,\n",
       "    0.6464004516601562,\n",
       "    0.8005459308624268,\n",
       "    0.6953597068786621,\n",
       "    0.8766813278198242,\n",
       "    0.7099354267120361,\n",
       "    0.7232522964477539,\n",
       "    0.806626558303833],\n",
       "   'tCentroids': [12.663001775741577,\n",
       "    19.51159381866455,\n",
       "    28.317543029785156,\n",
       "    32.34527087211609,\n",
       "    38.656880378723145,\n",
       "    46.827125549316406,\n",
       "    56.31013226509094,\n",
       "    57.457207679748535,\n",
       "    68.31060099601746,\n",
       "    71.4232063293457,\n",
       "    82.15160608291626,\n",
       "    85.98417043685913,\n",
       "    95.7277398109436],\n",
       "   'CostInit': [891555.5554243803,\n",
       "    22377.653392100456,\n",
       "    8629.214899629782,\n",
       "    5436.296716977931,\n",
       "    4260.795307674229,\n",
       "    3693.4135514749714,\n",
       "    2972.4869914393166,\n",
       "    2540.8690295473752,\n",
       "    2254.4309990245597,\n",
       "    1924.4365082423058,\n",
       "    1804.3187924517606,\n",
       "    1679.928830120031,\n",
       "    1566.4870302624386,\n",
       "    1457.8910337457664],\n",
       "   'tTotal': 818.8865065574646},\n",
       "  'Number of partition32': {'tSamples': [0.9800570011138916,\n",
       "    0.8967080116271973,\n",
       "    0.8486120700836182,\n",
       "    0.882718563079834,\n",
       "    0.8753995895385742,\n",
       "    0.829594612121582,\n",
       "    0.8131394386291504,\n",
       "    0.949406623840332,\n",
       "    0.8689103126525879,\n",
       "    0.8476817607879639,\n",
       "    0.8226644992828369,\n",
       "    0.9605312347412109,\n",
       "    0.6979317665100098,\n",
       "    0.848773717880249],\n",
       "   'tCentroids': [16.321369409561157,\n",
       "    22.282538175582886,\n",
       "    31.617116451263428,\n",
       "    36.274001359939575,\n",
       "    46.511996269226074,\n",
       "    55.12869882583618,\n",
       "    56.06206774711609,\n",
       "    64.05211472511292,\n",
       "    69.1995062828064,\n",
       "    74.88440203666687,\n",
       "    82.83336639404297,\n",
       "    91.18759965896606,\n",
       "    97.58591675758362,\n",
       "    105.72941970825195],\n",
       "   'CostInit': [1601525.631138093,\n",
       "    70982.98463394282,\n",
       "    12854.129735744746,\n",
       "    7203.102566750421,\n",
       "    5039.777206241083,\n",
       "    3782.8190417370442,\n",
       "    3008.7965252298245,\n",
       "    2608.265009099853,\n",
       "    2319.4600205730303,\n",
       "    2106.297739951042,\n",
       "    1920.2566548406478,\n",
       "    1718.7798349960303,\n",
       "    1549.2635994896345,\n",
       "    1445.8953639030187,\n",
       "    1371.7720844645594],\n",
       "   'tTotal': 980.8802380561829},\n",
       "  'Number of partition64': {'tSamples': [1.3998363018035889,\n",
       "    2.333587884902954,\n",
       "    1.0383284091949463,\n",
       "    1.248624324798584,\n",
       "    1.284059762954712,\n",
       "    1.108712911605835,\n",
       "    1.2358744144439697,\n",
       "    1.1507201194763184,\n",
       "    1.1776132583618164,\n",
       "    1.3565161228179932,\n",
       "    1.3936538696289062,\n",
       "    1.330669641494751,\n",
       "    1.1656172275543213],\n",
       "   'tCentroids': [14.460254430770874,\n",
       "    23.23945426940918,\n",
       "    32.2976348400116,\n",
       "    39.66645526885986,\n",
       "    45.06172728538513,\n",
       "    50.39931869506836,\n",
       "    58.97108602523804,\n",
       "    67.47464942932129,\n",
       "    75.48974418640137,\n",
       "    78.8404586315155,\n",
       "    87.32318902015686,\n",
       "    95.3143196105957,\n",
       "    103.69666767120361],\n",
       "   'CostInit': [1062852.2618626256,\n",
       "    56283.523745564045,\n",
       "    11259.154768067461,\n",
       "    6406.763363665761,\n",
       "    4258.798369865063,\n",
       "    3272.05091118598,\n",
       "    2885.6470398262263,\n",
       "    2349.998481071997,\n",
       "    2109.8747417259196,\n",
       "    1926.7860604089453,\n",
       "    1719.387112374389,\n",
       "    1598.2542540324723,\n",
       "    1443.2573331264082,\n",
       "    1351.768510414011],\n",
       "   'tTotal': 907.9069976806641},\n",
       "  'Number of partition128': {'tSamples': [1.9398612976074219,\n",
       "    1.731004238128662,\n",
       "    1.8601210117340088,\n",
       "    1.8389818668365479,\n",
       "    1.8242743015289307,\n",
       "    1.8414041996002197,\n",
       "    2.121408700942993,\n",
       "    2.450037956237793,\n",
       "    1.9219880104064941,\n",
       "    1.8956823348999023,\n",
       "    1.713805913925171,\n",
       "    1.796037197113037,\n",
       "    1.8901896476745605],\n",
       "   'tCentroids': [17.092105865478516,\n",
       "    26.234716415405273,\n",
       "    32.86576175689697,\n",
       "    40.274641036987305,\n",
       "    48.85936450958252,\n",
       "    57.13522124290466,\n",
       "    64.82174038887024,\n",
       "    71.61310529708862,\n",
       "    79.64548707008362,\n",
       "    84.1491162776947,\n",
       "    92.58800148963928,\n",
       "    99.59088397026062,\n",
       "    108.227543592453],\n",
       "   'CostInit': [843029.9190250103,\n",
       "    41242.46035814129,\n",
       "    11618.857152040846,\n",
       "    6515.874452333205,\n",
       "    4379.615086935071,\n",
       "    3232.9190282315076,\n",
       "    2665.7193403727592,\n",
       "    2272.009269748305,\n",
       "    2089.131106168153,\n",
       "    1877.3489408572775,\n",
       "    1723.008422719009,\n",
       "    1579.400790281332,\n",
       "    1447.6151884287149,\n",
       "    1358.6836763880476],\n",
       "   'tTotal': 978.1837546825409}},\n",
       " 'totalLogParallelKmeans': {'Number of partition8': {'CostsKmeans': [14491.25799091136,\n",
       "    14427.780944459158,\n",
       "    14379.067008154318,\n",
       "    14340.9031047214,\n",
       "    14326.827877044685],\n",
       "   'tIterations': [11.779228448867798,\n",
       "    9.766197919845581,\n",
       "    10.539317607879639,\n",
       "    10.567087888717651,\n",
       "    10.841463088989258],\n",
       "   'tTotal': 53.49331045150757},\n",
       "  'Number of partition4': {'CostsKmeans': [14154.331052221676,\n",
       "    14116.365971069381,\n",
       "    14112.281310880897],\n",
       "   'tIterations': [13.848233938217163, 12.48716115951538, 13.067476987838745],\n",
       "   'tTotal': 39.40287971496582},\n",
       "  'Number of partition2': {'CostsKmeans': [12473.205641309907,\n",
       "    12366.496092733309,\n",
       "    12331.738021866051,\n",
       "    12306.192485847434,\n",
       "    12291.58355890608,\n",
       "    12289.572628760156],\n",
       "   'tIterations': [27.306171655654907,\n",
       "    24.897124767303467,\n",
       "    25.204771995544434,\n",
       "    25.39222741127014,\n",
       "    24.535747289657593,\n",
       "    25.696465015411377],\n",
       "   'tTotal': 153.03252053260803},\n",
       "  'Number of partition16': {'CostsKmeans': [20135.80143345077,\n",
       "    20077.17761306065,\n",
       "    19954.853090147248,\n",
       "    19481.940068563963,\n",
       "    19367.454492017652,\n",
       "    19142.548174496766,\n",
       "    19083.416618553074,\n",
       "    19066.139849626437],\n",
       "   'tIterations': [10.880249977111816,\n",
       "    11.704259395599365,\n",
       "    10.999428272247314,\n",
       "    11.290096044540405,\n",
       "    10.57009744644165,\n",
       "    9.916236162185669,\n",
       "    10.009653091430664,\n",
       "    10.275928258895874],\n",
       "   'tTotal': 85.64597034454346},\n",
       "  'Number of partition32': {'CostsKmeans': [14126.406907854667,\n",
       "    14033.887584557884,\n",
       "    13959.66936071383,\n",
       "    13834.257777667352,\n",
       "    13786.206960802598,\n",
       "    13705.417893725662,\n",
       "    13658.396304373387,\n",
       "    13605.01363918185,\n",
       "    13531.854086231762,\n",
       "    13353.599330872743,\n",
       "    12909.174485262909,\n",
       "    12788.849158969733,\n",
       "    12761.957357692343,\n",
       "    12757.83950672009],\n",
       "   'tIterations': [12.061691761016846,\n",
       "    11.586308240890503,\n",
       "    11.568902015686035,\n",
       "    11.38906478881836,\n",
       "    11.316141366958618,\n",
       "    11.46327018737793,\n",
       "    11.998432636260986,\n",
       "    13.350913286209106,\n",
       "    11.131362438201904,\n",
       "    10.87834095954895,\n",
       "    17.41956663131714,\n",
       "    11.066790103912354,\n",
       "    11.18279504776001,\n",
       "    11.147577285766602],\n",
       "   'tTotal': 167.56118655204773},\n",
       "  'Number of partition64': {'CostsKmeans': [13002.761720190696,\n",
       "    12946.280623463417,\n",
       "    12858.600492427484,\n",
       "    12856.29847669335],\n",
       "   'tIterations': [13.89802074432373,\n",
       "    12.171608209609985,\n",
       "    12.45176911354065,\n",
       "    11.89674425125122],\n",
       "   'tTotal': 50.41814970970154},\n",
       "  'Number of partition128': {'CostsKmeans': [13256.068171158195,\n",
       "    13215.004392704688,\n",
       "    13199.35080389415,\n",
       "    13159.125690088671,\n",
       "    12995.72416396117,\n",
       "    12716.594692734348,\n",
       "    12702.29249101154,\n",
       "    12699.56508101606],\n",
       "   'tIterations': [16.877744674682617,\n",
       "    16.08725643157959,\n",
       "    15.860199928283691,\n",
       "    15.853113889694214,\n",
       "    15.62473440170288,\n",
       "    16.735519886016846,\n",
       "    15.63037919998169,\n",
       "    15.941917419433594],\n",
       "   'tTotal': 128.61088252067566}},\n",
       " 'tDurationsParallel': {'Number of partition8': 876.2816195487976,\n",
       "  'Number of partition4': 1718.7826912403107,\n",
       "  'Number of partition2': 2916.459883928299,\n",
       "  'Number of partition16': 915.2583429813385,\n",
       "  'Number of partition32': 1161.9603824615479,\n",
       "  'Number of partition64': 972.8995172977448,\n",
       "  'Number of partition128': 1126.6152312755585},\n",
       " 'tPreOperationsParallel': {'Number of partition8': 10.530800104141235,\n",
       "  'Number of partition4': 12.929564952850342,\n",
       "  'Number of partition2': 18.64969515800476,\n",
       "  'Number of partition16': 10.725661039352417,\n",
       "  'Number of partition32': 13.518644094467163,\n",
       "  'Number of partition64': 14.573980569839478,\n",
       "  'Number of partition128': 19.820308446884155}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ef580eb-8b27-4f84-b945-ef50a91bd244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'totalLogNaiveInit': {'Number of partition8': {'tTotal': 1.82527756690979},\n",
       "  'Number of partition4': {'tTotal': 2.5076260566711426},\n",
       "  'Number of partition2': {'tTotal': 3.5396792888641357},\n",
       "  'Number of partition16': {'tTotal': 1.9742786884307861},\n",
       "  'Number of partition32': {'tTotal': 2.4714605808258057},\n",
       "  'Number of partition64': {'tTotal': 3.1421775817871094},\n",
       "  'Number of partition128': {'tTotal': 5.65146279335022}},\n",
       " 'totalLogNaiveKmeans': {'Number of partition8': {'CostsKmeans': [82567.0250963437,\n",
       "    33104.67062765449,\n",
       "    31797.19712773952,\n",
       "    30862.86104723941,\n",
       "    30695.516272739696,\n",
       "    30633.602145267392,\n",
       "    30564.992296609005,\n",
       "    30482.895593478763,\n",
       "    30440.947605577367,\n",
       "    30416.90273136094],\n",
       "   'tIterations': [9.358434200286865,\n",
       "    8.255417346954346,\n",
       "    7.617798089981079,\n",
       "    7.953901290893555,\n",
       "    8.634777307510376,\n",
       "    9.0293550491333,\n",
       "    8.520485401153564,\n",
       "    8.888786792755127,\n",
       "    8.818779230117798,\n",
       "    8.965948820114136],\n",
       "   'tTotal': 86.04372000694275},\n",
       "  'Number of partition4': {'CostsKmeans': [155320.87780776434,\n",
       "    69808.04708340974,\n",
       "    54725.117012482755,\n",
       "    43558.38585393489,\n",
       "    42153.973898581025,\n",
       "    39892.612618343206,\n",
       "    36677.7784101257,\n",
       "    36601.76233647565,\n",
       "    36533.77835639385,\n",
       "    36487.0518173548,\n",
       "    36465.68111002703],\n",
       "   'tIterations': [11.667833089828491,\n",
       "    9.159595251083374,\n",
       "    8.723095893859863,\n",
       "    8.762112855911255,\n",
       "    8.907129526138306,\n",
       "    9.004110336303711,\n",
       "    8.768619537353516,\n",
       "    8.80795955657959,\n",
       "    8.801142930984497,\n",
       "    8.743201732635498,\n",
       "    8.798418521881104],\n",
       "   'tTotal': 100.14324736595154},\n",
       "  'Number of partition2': {'CostsKmeans': [42516.03346356709,\n",
       "    30489.899569214176,\n",
       "    28870.557007268813,\n",
       "    27552.014438794737,\n",
       "    23278.24060503552,\n",
       "    18984.3147713026,\n",
       "    18760.240816989506,\n",
       "    18693.13101953215,\n",
       "    18670.316806382605,\n",
       "    18655.67316314641],\n",
       "   'tIterations': [23.22072458267212,\n",
       "    21.562345504760742,\n",
       "    21.96597170829773,\n",
       "    21.554337978363037,\n",
       "    21.63866353034973,\n",
       "    21.29712414741516,\n",
       "    21.602866649627686,\n",
       "    21.09733819961548,\n",
       "    21.08929681777954,\n",
       "    21.192963123321533],\n",
       "   'tTotal': 216.22165393829346},\n",
       "  'Number of partition16': {'CostsKmeans': [166384.53462033137,\n",
       "    65895.60563070832,\n",
       "    65706.02173432914,\n",
       "    65383.54430620917,\n",
       "    52128.157728599246,\n",
       "    45756.44843686664,\n",
       "    43376.97766168072,\n",
       "    42947.12896710763,\n",
       "    42930.39892842657],\n",
       "   'tIterations': [9.774840593338013,\n",
       "    9.00724983215332,\n",
       "    7.818156719207764,\n",
       "    7.3633623123168945,\n",
       "    8.3872971534729,\n",
       "    8.063755750656128,\n",
       "    8.452018022537231,\n",
       "    7.990036725997925,\n",
       "    7.707749605178833],\n",
       "   'tTotal': 74.56449055671692},\n",
       "  'Number of partition32': {'CostsKmeans': [49520.386358362106,\n",
       "    30527.58542146165,\n",
       "    23020.67136604504,\n",
       "    21911.733491508534,\n",
       "    21754.20134667065,\n",
       "    21737.064097228238],\n",
       "   'tIterations': [10.73610258102417,\n",
       "    9.135873317718506,\n",
       "    9.866609334945679,\n",
       "    8.899936199188232,\n",
       "    8.921936511993408,\n",
       "    9.472395420074463],\n",
       "   'tTotal': 57.03286814689636},\n",
       "  'Number of partition64': {'CostsKmeans': [139644.47538727487,\n",
       "    36535.13405139362,\n",
       "    33452.63488532336,\n",
       "    32223.7184884443,\n",
       "    31195.288436264193,\n",
       "    31096.484073064148,\n",
       "    31059.985512694107,\n",
       "    30952.398379187733,\n",
       "    30697.26167899626,\n",
       "    30519.036479392384,\n",
       "    30363.737355164907,\n",
       "    30089.8334407707,\n",
       "    29614.347467224787,\n",
       "    29549.847534218025,\n",
       "    29539.846943083],\n",
       "   'tIterations': [11.088544845581055,\n",
       "    9.938262939453125,\n",
       "    10.427467346191406,\n",
       "    9.227502346038818,\n",
       "    10.919835567474365,\n",
       "    9.76926326751709,\n",
       "    9.618249416351318,\n",
       "    9.622366666793823,\n",
       "    9.751763343811035,\n",
       "    9.964595556259155,\n",
       "    9.911336183547974,\n",
       "    10.541150331497192,\n",
       "    9.954270601272583,\n",
       "    9.533994913101196,\n",
       "    9.533462762832642],\n",
       "   'tTotal': 149.80209684371948},\n",
       "  'Number of partition128': {'CostsKmeans': [37588.1663885156,\n",
       "    26849.463446228598,\n",
       "    25295.796181492984,\n",
       "    24023.460059451285,\n",
       "    22624.616949744275,\n",
       "    22344.19222911269,\n",
       "    22278.733899910447,\n",
       "    22222.518277534593,\n",
       "    22182.3281202257,\n",
       "    22048.96907256953,\n",
       "    21939.825265074447,\n",
       "    21863.338224566698,\n",
       "    21551.51639223278,\n",
       "    21176.68219891586,\n",
       "    20696.410448243634,\n",
       "    20654.242605739484,\n",
       "    20646.015374805942],\n",
       "   'tIterations': [14.191697835922241,\n",
       "    13.370175123214722,\n",
       "    13.394645690917969,\n",
       "    13.088303089141846,\n",
       "    12.958121538162231,\n",
       "    13.09688687324524,\n",
       "    14.682480573654175,\n",
       "    13.33041501045227,\n",
       "    13.132146835327148,\n",
       "    13.155274868011475,\n",
       "    13.28964638710022,\n",
       "    13.04693865776062,\n",
       "    13.236061811447144,\n",
       "    13.329109191894531,\n",
       "    13.32405710220337,\n",
       "    13.168695449829102,\n",
       "    12.749795913696289],\n",
       "   'tTotal': 226.54448246955872}},\n",
       " 'tDurationsNaive': {'Number of partition8': 97.91845202445984,\n",
       "  'Number of partition4': 113.30451202392578,\n",
       "  'Number of partition2': 235.50393104553223,\n",
       "  'Number of partition16': 86.43284273147583,\n",
       "  'Number of partition32': 69.03074526786804,\n",
       "  'Number of partition64': 163.41193318367004,\n",
       "  'Number of partition128': 246.70958375930786},\n",
       " 'tPreOperationsNaive': {'Number of partition8': 10.049140214920044,\n",
       "  'Number of partition4': 10.653437614440918,\n",
       "  'Number of partition2': 15.74240779876709,\n",
       "  'Number of partition16': 9.8938889503479,\n",
       "  'Number of partition32': 9.526236057281494,\n",
       "  'Number of partition64': 10.467516660690308,\n",
       "  'Number of partition128': 14.513504028320312}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logNaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be8588-13da-4d15-b551-3b64462412b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill spark and the context\n",
    "sc.stop()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17473b2-7f02-43a5-81fa-b970fc813216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
