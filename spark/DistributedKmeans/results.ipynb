{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e331f9-7a83-4d67-9dc3-72dfcabc8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898e438-13ed-44b5-810e-9b67228ef6a7",
   "metadata": {},
   "source": [
    "# SET UP SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0765e523-fef5-46d0-98fb-7e14f63eb1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/04 13:40:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My first spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd94c825-c1ed-4258-95d8-12e1908b12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da21a9-4d6b-4a76-9fcf-a8c126204029",
   "metadata": {},
   "source": [
    "# DESCRIBE THE DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38dfb87a-1c22-41e7-94c4-130931a0098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_kddcup99(return_X_y = True, percent10 = True) # default percent10=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad08fa15-dd00-4405-ad5b-c13602aa6c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples is  494021\n",
      "The dimensionality is  41\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of samples is \", data[0].shape[0])\n",
    "print(\"The dimensionality is \", data[0].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ae9ff-2fe1-4b8f-ad78-3dff1f9498d4",
   "metadata": {},
   "source": [
    "Classes: 23\n",
    "Samples total: 4898431\n",
    "Dimensionality: 41\n",
    "Features: discrete (int) or continuous (float)\n",
    "\n",
    "The first containing a 2D array of shape (n_samples, n_features) with each row representing one sample and each column representing the features\n",
    "\n",
    "\n",
    "The first 4 elements of the data set (letters, what are them)\n",
    "Duration: The length (in seconds) of the connection.\n",
    "Protocol Type: The type of protocol used, such as TCP, UDP, or ICMP.\n",
    "Service: The network service on the destination, such as HTTP, FTP, or SMTP.\n",
    "Flag: The status flag of the connection, such as SF (successful connection), REJ (connection rejected), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3744860-d864-4b7c-adb0-d83eba8adbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect samples and features (target)\n",
    "x = data[0]\n",
    "y = data[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64090da9-465f-457f-8826-09d9f45275cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the data fro memory reasons\n",
    "subLen = 1000\n",
    "x = x[:subLen,]\n",
    "y = y[:subLen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0270ffce-5fd8-4bc3-bb73-9eb03e9bafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle over num_slices to be run on cloud veneto on a .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fd8793-acb2-48f6-a3b8-579308cf33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nSlice = None # IMPORTANT PARAMETER FOR NUMBER OF PARTITIONS\n",
    "Rdd = sc.parallelize([(None, {\"x\": x[i],\"y\": y[i], \"d2\":None}) for i in range(len(y))], numSlices = nSlice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab5cc4d-60d9-4c75-9e1d-ee385d908c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdd.getNumPartitions() # check partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9c39c-6d39-43b4-89a2-d6ea18e92422",
   "metadata": {},
   "source": [
    "Check the symbolic variables in the dataset to have only usefull data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1063dc5e-e86b-4caf-bdad-e6ac24923ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "typeElement = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "                 .map(lambda x: [set([type(x[i])]) for i in range(len(x))])\\\n",
    "                 .reduce(lambda a, b: [a[i].union(b[i]) for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856778f6-14ba-4798-b8a8-3632bcdc66e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The types of the dimensionalities are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{int},\n",
       " {bytes},\n",
       " {bytes},\n",
       " {bytes},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {int},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {int},\n",
       " {int},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float},\n",
       " {float}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ADD TABLES WITH MATPLOTLIB\n",
    "print(\"The types of the dimensionalities are:\")\n",
    "typeElement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63fe7b-77dd-4955-8013-b35d67733ddf",
   "metadata": {},
   "source": [
    "Look for the unique variables over the attributes using parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb06173-f46a-416d-bda9-a422ef9bd1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/04 13:42:33 WARN TaskSetManager: Lost task 14.0 in stage 2.0 (TID 47) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ExceptionInInitializerError: Cannot unpack libzstd-jni-1.5.5-4: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat com.github.luben.zstd.util.Native.load(Native.java:140)\n",
      "\tat com.github.luben.zstd.util.Native.load(Native.java:85)\n",
      "\tat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.<clinit>(ZstdOutputStreamNoFinalizer.java:18)\n",
      "\tat com.github.luben.zstd.RecyclingBufferPool.<clinit>(RecyclingBufferPool.java:18)\n",
      "\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n",
      "\t... 36 more\n",
      "\n",
      "24/07/04 13:42:33 ERROR TaskSetManager: Task 3 in stage 2.0 failed 4 times; aborting job\n",
      "24/07/04 13:42:33 WARN TaskSetManager: Lost task 10.3 in stage 2.0 (TID 57) (10.67.22.136 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 56) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n",
      "\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n",
      "\t... 36 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/04 13:42:33 WARN TaskSetManager: Lost task 2.0 in stage 2.0 (TID 45) (10.67.22.136 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 56) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n",
      "\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n",
      "\t... 36 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/04 13:42:33 WARN TaskSetManager: Lost task 7.0 in stage 2.0 (TID 36) (10.67.22.136 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 56) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n",
      "\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n",
      "\t... 36 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/04 13:42:33 WARN TaskSetManager: Lost task 11.0 in stage 2.0 (TID 40) (10.67.22.136 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 56) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n",
      "\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n",
      "\t... 36 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/04 13:42:33 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 32) (10.67.22.136 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 56) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n",
      "\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n",
      "\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n",
      "\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n",
      "\t... 36 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 56) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n\t... 36 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 2044\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 56) (10.67.22.136 executor 2): java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:88)\n\tat org.apache.spark.MapOutputTracker$.deserializeObject$1(MapOutputTracker.scala:1570)\n\tat org.apache.spark.MapOutputTracker$.deserializeOutputStatuses(MapOutputTracker.scala:1585)\n\tat org.apache.spark.MapOutputTrackerWorker.$anonfun$getStatuses$7(MapOutputTracker.scala:1461)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.MapOutputTrackerWorker.getStatuses(MapOutputTracker.scala:1454)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1319)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1289)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool\n\tat org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:228)\n\t... 36 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uniquesParallel = []\n",
    "for i in range(41):\n",
    "    s = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "        .map(lambda x: x[i])\\\n",
    "        .distinct()\\\n",
    "        .count()\n",
    "    uniquesParallel.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f044ca78-926f-4bfc-8a42-6869c14c1218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of uniques is []\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of uniques is\", uniquesParallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffad4d-9e3b-4886-86f7-bded6bf94366",
   "metadata": {},
   "source": [
    "Look for the unique variables over the attributes in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c374f8-3dfb-41f8-a326-ec943ebebf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "uniques = []\n",
    "for i in range (x.shape[1]):\n",
    "    k = (len(np.unique(x[:, i])))\n",
    "    uniques.append(k)\n",
    "print(\"The uniques are:\", uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d3067-5179-4bfe-a5f4-7022a9fc80e7",
   "metadata": {},
   "source": [
    "It takes 4 times for to do it in parallel (explain what is better to use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a9979-a069-451b-91fd-9b1caf0e6fab",
   "metadata": {},
   "source": [
    "why running it a second time takes way more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aba093-9df5-4318-8386-fca8982f209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kTrue = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "        .distinct()\\\n",
    "        .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05939b-172b-49f5-b304-1ed9ae9a151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The true number of the classes is\", kTrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341f52d-c90e-4238-88eb-a3c117145f9c",
   "metadata": {},
   "source": [
    "Unique number of labels in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c871b-755c-4d52-9719-0323c7ea254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "uniquesParallely = Rdd.map(lambda datum: datum[1][\"y\"])\\\n",
    "        .countByValue()\n",
    "\n",
    "print(\"The y things are:\", uniquesParallely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969e1c8-e14f-4d01-8991-c8c92dbd4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "yUnique = [a.decode('utf-8') for a in uniquesParallely.keys()]\n",
    "xUnique = list(uniquesParallely.values())\n",
    "attributes = [\n",
    "    \"duration\",\n",
    "    \"protocol_type\",\n",
    "    \"service\",\n",
    "    \"flag\",\n",
    "    \"src_bytes\",\n",
    "    \"dst_bytes\",\n",
    "    \"land\",\n",
    "    \"wrong_fragment\",\n",
    "    \"urgent\",\n",
    "    \"hot\",\n",
    "    \"num_failed_logins\",\n",
    "    \"logged_in\",\n",
    "    \"num_compromised\",\n",
    "    \"root_shell\",\n",
    "    \"su_attempted\",\n",
    "    \"num_root\",\n",
    "    \"num_file_creations\",\n",
    "    \"num_shells\",\n",
    "    \"num_access_files\",\n",
    "    \"num_outbound_cmds\",\n",
    "    \"is_host_login\",\n",
    "    \"is_guest_login\",\n",
    "    \"count\",\n",
    "    \"srv_count\",\n",
    "    \"serror_rate\",\n",
    "    \"srv_serror_rate\",\n",
    "    \"rerror_rate\",\n",
    "    \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\n",
    "    \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\",\n",
    "    \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\",\n",
    "    \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\",\n",
    "    \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6636680-4beb-49c8-8472-b0175903349b",
   "metadata": {},
   "source": [
    "Plot of the count of the ocurency of the uniques of label and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0c92c-38d7-41bb-9899-86e3c224004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add log 10 the ax and npt the count\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].barh(yUnique, np.log10(xUnique))\n",
    "ax[0].set_xlabel(\"log10 count\")\n",
    "ax[0].set_ylabel(\"Label\")\n",
    "ax[0].set_title(\"Label count\")\n",
    "ax[1].barh(attributes, np.log10(uniquesParallel))\n",
    "ax[1].set_xlabel(\"log10 count\")\n",
    "ax[1].set_ylabel(\"Attribute\")\n",
    "ax[1].set_title(\"Attribute count\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41a645-6a8f-4049-99ce-26e68d3d34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteBytes(datum):\n",
    "    x = datum[1][\"x\"]\n",
    "    mask = [type(i) != bytes for i in x]\n",
    "    datum[1][\"x\"] = np.asarray(x[mask])\n",
    "    print(x)\n",
    "    print(mask)\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d499162-00b8-4c32-a321-ad9f502a4283",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rdd = Rdd.map(deleteBytes)\\\n",
    "         .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3b744-ee86-471f-a0e3-f708c3934416",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberColumns = len([i for i in typeElement if i != set([bytes])])\n",
    "print(\"The number of columns (dimensionality) after filtering the bytes is\", numberColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2d0e1-ded1-4916-9874-5662030818e4",
   "metadata": {},
   "source": [
    "# PARALLEL PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d37d2-c8b3-479f-be71-39eca4626f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "maxS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.maximum(a, b))\n",
    "minS = Rdd.map(lambda datum: datum[1][\"x\"])\\\n",
    "           .reduce(lambda a, b: np.minimum(a, b))\n",
    "\n",
    "Rdd = Rdd.map(lambda datum: minmaxRescale(datum, minS, maxS))\\\n",
    "         #.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e75bc7-7143-4523-8680-4ebfbb0ac8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logParallelInit = {}\n",
    "logParallelKmeans = {}\n",
    "\n",
    "k=kTrue\n",
    "l=k*2 # rescaling probability to have more centroids than k\n",
    "\n",
    "# inizialize the centroids with kmeans parallel and compute C after the iterations\n",
    "C_init = parallelInit(Rdd, k, l, logParallelInit)\n",
    "C = kMeans(Rdd, C_init, 15, logParallelKmeans)\n",
    "Rdd=Rdd.map(lambda datum: selectCluster(datum, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa35a8-26ba-4ebc-aced-ff9a38ea1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1,3, figsize=(15,10))\n",
    "ax[0].plot(logParallelInit[\"tCentroids\"])\n",
    "ax[1].plot(logParallelInit[\"tSamples\"])\n",
    "ax[2].plot(logParallelInit[\"CostInit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da414962-608b-4902-8b27-d2834cd619f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logParallelKmeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlogParallelKmeans\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtIterations\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(logParallelKmeans[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCostsKmeans\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logParallelKmeans' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax=plt.subplots(1,2, figsize=(15,10))\n",
    "ax[0].plot(logParallelKmeans[\"tIterations\"])\n",
    "ax[1].plot(logParallelKmeans[\"CostsKmeans\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
